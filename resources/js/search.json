[[{"i":"what-is-pygmalionai","l":"What is PygmalionAI?","p":["PygmalionAI is an open-source large language model ( LLM) based on EleutherAI's GPT-J 6B.","In simple terms, Pygmalion is an AI fine-tuned for chatting and roleplaying purposes. The current actively supported Pygmalion AI model is the 6B variant, a model with 6 billion parameters."]},{"l":"Features","p":["Unrestricted: No measures have been put in place to restrict the output.","Low VRAM requirement: With only 16GB (or less) VRAM required, Pygmalion offers better chat capability than much larger language models with relatively minimal resources.","Fine-tuned for RP: Our curated dataset of high-quality roleplaying data ensures that your bot will be the optimal RP partner.","Free and Open-Source: Both the model weights and the code used to train it are completely open-source, and you can modify/re-distribute it for whatever purpose you want.*","Regularly updated: Pygmalion is regularly updated with new data to improve the model even further.","* PygmalionAI uses the CreativeML Open RAIL-M license, which is also used by Stable Diffusion."]},{"i":"how-do-i-use-pygmalion","l":"How do I use Pygmalion?","p":["Language models, including Pygmalion, generally run on GPUs since they need access to fast memory and massive processing power in order to output coherent text at an acceptable speed. Pygmalion is no different. You need a powerful GPU to run the model.","If you don't have a powerful enough GPU, there are luckily alternative solutions. Google's Colaboratory offers free GPUs for all users - though for a limited time. We have ready-made notebooks that you can run to access Pygmalion. You can also use Colab on your mobile phone.","These docs will attempt to explain all the possible methods for running the AI, and in a language that is easy to understand. Please navigate to the next section to begin using Pygmalion."]}],[{"l":"Quickstart","p":["There are multiple ways to run a large language model like Pygmalion, including using cloud computing instances or locally installed software. For users who want to run the model locally, it's important to ensure that their computer meets the minimum requirements, including having a GPU with enough memory. Outlined below are instructions for Windows Users to find out which criteria they meet so they can quickly get started."]},{"l":"Windows"},{"l":"Identifying your GPU","p":["Find out what GPU you're using (if any at all) by performing the following steps:","Press the Windows Key+ R. This will open the Run window.","Type in dxdiag and press enter.","You might be greeted with a dialogue box if it's the first time launcing dxdiag. You can pick whatever option you want, it doesn't matter.","Look for the Display tab on the top side of the window. Depending on your PC/Laptop, there might be two Display tabs - aptly named \"Display 1\" and \"Display 2\". In that case, click on \"Display 2\" instead.","Look for the `Display Memory (VRAM) part of the screen.","Proceed to the next section","Unfortunately, the framework required to enable compute on AMD GPUs (ROCm) is currently unavailable on Windows. While we wait for AMD to add ROCm support to Windows, you will either have to switch to Linux or use the Cloud/C++ solutions."]},{"l":"Linux"},{"i":"identifying-your-gpu-1","l":"Identifying your GPU","p":["Use the following command to find out whether you have a discrete GPU or not:","If you have a dedicated GPU, you should see something like this; an NVIDIA card with the Kernel drive in use being nvidia:","In that case, run nvidia-smi to view your VRAM amount. Then continue to the next section. If the kernel driver in use is not nvidia or nvidia-lts, you will need to install the nvidia drivers for your distro.","If you don't have a discrete GPU, you would see something like this: In that case, proceed to this section"]},{"i":"what-to-do-now","l":"What to do now?","p":["There are several options, depending on how much VRAM your system has. Outlined below are the multiple scenarios and the recommended method of running Pygmalion for each. Please use the Table of Contents for navigating."]},{"i":"i-have-no-vram","l":"I have no VRAM!","p":["In this case, your only choice is to either run on the Cloud or use Pygmalion C++ to run the model on your CPU."]},{"l":"Cloud","p":["There are several options for running on the cloud - Google Colab and GPU rental services.","Google Colab is free, but there are restrictions for the end user. Namely: daily/weekly usage quotas and a less powerful GPU. If you want to take this route, please refer to either the TextGen WebUI for running Pygmalion itself or KoboldAI Notebooks for the various Mixed Models.","GPU Rental services, such as vast.ai are another option. They're paid services, but with the added benefits of powerful GPUs, virtually no quotas (since you pay hourly and on-the-go), and next to no restrictions on your usage. The included guide for vast.ai in these docs provide a docker image for KoboldAI which will make running Pygmalion very simple for the average user."]},{"i":"pygmalion-c","l":"Pygmalion C++","p":["If you have a decent CPU, you can run Pygmalion with no GPU required, using the GGML framework for Machine Learning. Please refer to the guide for instructions on how to proceed."]},{"i":"i-have-less-than-4gb","l":"I have less than 4GB!","p":["In that case, please refer to the section above for Cloud and C++ options."]},{"i":"i-have-6gb-or-more-but-less-than-10gb","l":"I have 6GB or more but less than 10GB!","p":["Please refer to the 4-bit guide. Alternatively, you can also use one of the Cloud options if you find the 4bit model undesirable."]},{"i":"i-have-10gb-or-more-but-less-than-16gb","l":"I have 10GB or more but less than 16GB!","p":["Please refer to the TextGen WebUI guide to run Pygmalion at 8bit precision. Make sure you pass the --load-in-8bit argument when launching the WebUI. Alternatively, if you're using Linux, you can also use KoboldAI for 8-bit precision mode. Please refer to the 4-bit guide for instructions."]},{"i":"i-have-16gb-or-more","l":"I have 16GB or more!","p":["You have the recommended amount of VRAM for the 6B model. You may pick whatever option you want - all the guides here will work for you. You can get started here."]}],[{"l":"Frequently Asked Questions"},{"i":"how-do-i-use-the-latest-dev-v8-version","l":"How do I use the latest dev (v8) version?","p":["If you're running on Google Colab, all notebooks have an option for downloading the dev(beta) model. Look for either Pygmalion 6B Dev or Pygmalion 6B Experimental.","If you're running locally, make sure git is installed and run the following command inside your KoboldAI/models or text-generation-webui/models folder:","If it says git lfs was not recognized as a command, please install git-lfs with your package manager (Linux) or download the installer for Windows."]},{"i":"my-character-has-terrible-memory","l":"My character has terrible memory!","p":["This could be due to the limited context size. Pygmalion 6B has a maximum context size of 2048 tokens. This includes your character description, example messages, and all your chatlogs. The description and examples are placed at the top of the context memory - all your subsequent chat logs are placed beneath them. This means that if your character description + examples chats are 400 tokens, you'll only have 1648 tokens left for your messages. The bot will forget everything past those 1648 tokens.","Character editors/creators such as TavernAI's will give you a token count for your character, but you can also use OpenAI's tokenizer service.","OpenAI Tokenizer"]},{"i":"my-characters-responses-are-too-short","l":"My character's responses are too short!","p":["The character will mimic the example chats and greeting message's style. Keep these in mind if you want to create a verbose character:","Be descriptive and verbose in your greeting message and example chats.","Be descriptive and verbose in responses by the user (you!).","Re-generate the response until you see a satisfactory one.","Manually edit the character's response to make it more verbose."]},{"i":"my-character-is-generating-nonsense","l":"My character is generating nonsense!","p":["Your temperature or repetition penalty settings are likely too high. The recommended range for temperature (for chatbots) is between 0.5 to 0.9 and the ideal range for repetition penalty is between 1.1 to 1.2."]},{"i":"my-character-is-not-responding-anymore","l":"My character is not responding anymore!","p":["You have probably run out of GPU memory (also known as VRAM). If you are using the free Google Colab plan, the GPU provided by Google can't handle the maximum context size (2048 tokens). If you have 16GB of VRAM, please tweak the context size to 1400-1600 (proportionally lower according to your VRAM size)."]},{"i":"for-text-gen-oobabooga-you-need-to-check-the-load_in_8bit-option-under-the-3rd-cell-before-running-it","l":"For Text-Gen (Oobabooga) you need to check the load_in_8bit option under the 3rd cell before running it."},{"i":"for-tavernai-running-the-model-in-8bit-is-not-available-the-only-solution-is-to-slide-down-the-context-size-to-around-1400-or-1600-in-the-settings-tab","l":"For TavernAI, running the model in 8bit is not available. The only solution is to slide down the context size to around 1400 (or 1600) in the settings tab."},{"i":"do-you-keepshare-any-of-my-datachat","l":"Do you keep/share any of my data/chat?","p":["The Pygmalion project team does not collect any data other than the chat logs you explicitly consent to donating for the training dataset. This project is done out of passion, and no one has any intention of collecting, analyzing, or selling any of your data. The Colab notebook automatically passes the --quiet argument in KoboldAI, which means your/the bot's responses aren't printed out for Google to see."]},{"i":"what-are-softprompts-do-i-need-them","l":"What are Softprompts? Do I need them?","p":["A soft prompt is a way to modify the style and behavior of your AI.","They're made by taking a (possibly) huge amount of information and compressing them into a small number of tokens, then feeding it into the embedding layer of the model. Even if people tend to use it that way, softprompt are not made to add context, lore, etc to the story. But yes, it might work for that if the dataset use to make the softprompt was big enough and made use of good quality data."]},{"i":"how-do-i-use-softprompts","l":"How do I use Softprompts?","p":["Download your SP of choice from either the Discord or this Rentry a generous anon is maintaining.","If you're running locally: place the .zip file (don't extract!) inside the softprompts folder - both Oobabooga and KoboldAI have the same folder.","If you're using Google Colab, open your Google Drive and find the KoboldAI/softprompts folder and upload your .zip file there.","Make sure Pygmalion is loaded, then an option for Softprompts should appear."]},{"i":"which-ui-should-i-use-theres-so-many-of-them","l":"Which UI should I use? There's so many of them.","p":["It's up to you, every UI tends to add the same features others as well as try to be compatible with characters from other UIs as well. If you want to focus on story generation, we recommend using KoboldAI. For chat purposes, TavernAI has the best user interface, followed by Oobabooga. There are other TavernAI alternatives (e.g. miku.gg), but it's up to you in the end. Please refer to their respective pages for a preview of the UI.","--"]},{"i":"im-using-google-colab-how-much-time-do-i-have-to-use-their-gpus","l":"I'm using Google Colab, how much time do I have to use their GPUs?","p":["The amount of time you can use when using the free plan for Google Colab highly depends on the traffic Colab recieves and your usage patterns. Google says that you can use a Colab Notebook for at most 12 hours if you don't have Compute Units. Pygmalion users tend to say it's less - usually anywhere from two to six hours."]},{"i":"how-to-avoid-reaching-the-gpu-quota","l":"How to avoid reaching the GPU quota?","p":["When you're done using Pygmalion, please terminate your Colab session! You'll waste your quota otherwise, and might find yourself unable to connect to a GPU backend the next time you login."]},{"i":"ive-out-of-gpu-time-what-do-i-do-now","l":"I've out of GPU time, what do I do now?","p":["If you've run out of GPU time, you can do nothing but wait for your quota to reset. Alternatively, you can purchase Colab Pro to increase your daily/weekly quota."]},{"i":"whats-a-compute-unit-cu-should-i-buy-one","l":"What's a Compute Unit (CU)? Should I buy one?","p":["A CU is a unit that you can buy to be sure to be granted access to a powerful GPU. Pricing for Compute Engine is based on per-second usage of the machine types, persistent disks, and other resources that you select for your virtual machines. So it's up to you."]},{"i":"ive-run-out-of-quota-but-i-cant-buy-a-compute-unit-what-do-i-do","l":"I've run out of quota, but I can't buy a Compute Unit! What do I do?","p":["In that case, you can use Kobold Horde. Generous users are donating their GPU power to the Horde so that people can use them to run language models. Pygmalion is a popular model, so you'll always find people hosting it. Keep in mind that there are more Horde users than workers, so you might have to wait a bit longer for responses."]}],[{"l":"Colab Notebooks","p":["Google has effectively banned all mentions of PygmalionAI in their Colabs. As a result of this, Kobold and Tavern's official Colabs will no longer work. Oobabooga's Text Generation WebUI still works, so we'll have a guide for that included here.","As of April 24th 2023, Google has banned all the mixed models notebooks, so accessing Pygmalion is even more difficult. In light of this, we have removed all Google Colab links from the docs."]}],[{"l":"Vast AI","p":["Vast AI is a cloud computing service that provides high-end consumer and datacenter GPUs for very cheap prices. You can easily rent one of their many GPUs and use them to run Pygmalion AI."]},{"i":"why-vastai","l":"Why Vast.ai?","p":["Google Colab offers free GPUs, however, Google is quite restrictive about how the end-user makes use of their notebooks. They have already banned PygmalionAI specifically from their notebooks, so Colab users will be very limited in what they can do. Due to their cheap prices, vast.ai should be quite affordable for the majority of users."]},{"l":"Get started","p":["Use this url to set up a PygmalionAI docker template. This will install all the necessary requirements to run PygmalionAI via a docker image.","After opening the URL, you'll be prompted to sign up if it's your first time using vast.ai's services.","You'll be greeted with the Instances page, where you can select which GPU to use. For PygmalionAI, the cheaptest and best option is an NVIDIA RTX A5000 24GB, which allows for the maximum context size at 2048 tokens.","Make sure you set the Disk size to at least 40GB, and the price to the increasing order, as demonstrated in the screenshot above.","Assuming you've set up a billing plan, you should be able to immediately rent your desired machine and get started.","Click on \"Rent\" next to your selected GPU to get started.","Now, head on over to your Instances page and wait for your machine to setup everything for you.","Depending on the download speed of the machine you selected, it could take up to 5 minutes. If it's especially slow at 100mbps, it could potentially take 20 minutes. Make sure you select a machine with a fast internet connection.","Once it's finished loading, click on the logs button to view your remote url:","This process can take up to 3 minutes, so it's advised to close and re-open the logs window until the url shows up at the end.","Now, paste the URL in a browser tab and you're ready to use Kobold! You can load Pygmalion 6B by clicking on Models on the top-left corner of the screen, selecting the Chat models section, selecting Pygmalion 6B, and then clicking on Load. This will download the model for you. Keep in mind that the download is 16GB, but if you've chosen a machine with a fast download speed, it shouldn't take long. The steps should be familiar for users running locally, so you can also refer to the Local installation guide for Kobold.","You're now ready to use PygmalionAI! Make sure you delete (or stop, if you don't mind paying for the hourly storage charges of 0.8 cents/hr) your instance so you won't be billed when you're not using your instance. If you delete the instance, you'll have to go through the process of setting it up again, but stopping and resuming should pick up from where you left off."]},{"l":"Connecting your instance to TavernAI","p":["You can easily use TavernAI along with Kobold. Follow the instructions in the Tavern guide but instead of inputting localhost:5000/api, use your trycloudflare.com link. Assuming your URL was https://pieces-strictly-transparency-luther.trycloudflare.com/new_ui, you would paste it as https://pieces-strictly-transparency-luther.trycloudflare.com/api inside Tavern. Note that the new_ui part is removed and replaced with /api."]}],[{"l":"Overview","p":["16GB","20GB","24GB","AMD","Consumer-grade (Gaming) GPUs:","GPU","If you don't have any of these cards, but have still have 6GB or more VRAM, you can still run Pygmalion! (Provided you're using NVIDIA) Please refer to the Quickstart page.","Manufacturer","NVIDIA","Please continue to the next section.","PygmalionAI is a large language model, and as the name might imply, it needs a lot of computation power to run it. You will need a minimum of 16GB VRAM to run the model - without any optimizations, that is. Here's a list of all the GPUs that would work without any tweaks, out-of-the-box:","Radeon RX 6800","Radeon RX 6800 XT","Radeon RX 6900 XT","Radeon RX 6950 XT","Radeon RX 7900 XT","Radeon RX 7900 XTX","RTX 3090","RTX 3090 Ti","RTX 4080","RTX 4090","Titan RTX","VRAM"]}],[{"l":"Setting up your GPU","p":["Consumer-grade GPUs are designed for graphics processing and 3D rendering - they're not specifically made for language processing models. However, GPU vendors such as NVIDIA and AMD have released toolkits for enabling deep learning computation for their GPUs. Only a handful of AMD GPUs are supported, but the majority of NVIDIA cards can be set up easily.","Keep in mind that the most important factor for AI is VRAM (video memory), not the processing power. e.g. an RTX 2070 (8GB) is better than an RTX 4050 (6GB) when it comes to AI inference.","Please refer to the links below to check if your GPU is compatible:","-","If your GPU is supported, please proceed with the installation."]},{"l":"NVIDIA","p":["NVIDIA cards are mostly supported, though it's not recommended to use GPUs with less than 8GB of VRAM.","The setup process will be different for both Windows and Linux (MacOS currently not supported)."]},{"l":"Windows","p":["You'll need to install the CUDA Toolkit to enable compute on your NVIDIA GPU. You can download them here. Note that you should install version 11.7 CUDA, as the newer or older versions have been found to not work with KoboldAI."]},{"l":"Linux","p":["CUDA installation is relatively easy for Linux (unless you're on Fedora). For this particular guide, we'll be covering two base distros - Arch Linux and Debian. More will be added later."]},{"l":"Arch Linux","p":["Install the NVIDIA drivers (if they aren't already installed):","If you're on the LTS Arch Linux Kernel, then please install nvidia-lts instead. You can find what Kernel you're using by running uname -r","Install CUDA and (optionally) cuDNN:","You're done! Verify installation by running nvcc -v."]},{"l":"Debian","p":["Install the NVIDIA drivers (if they aren't already installed):","Install CUDA:","You're done! Verify installation by running nvcc -v."]},{"l":"AMD","p":["ROCm is (currently) not supported by Windows. You cannot run any language model on Windows if you have an AMD card - this isn't a Pygmalion-only issue. You will need to either dual-boot Linux or switch to Linux entirely if you wish to run language models on your AMD GPU."]},{"i":"linux-1","l":"Linux","p":["The ROCm installation is only (relatively) easy on Arch Linux. For other distros, you will need to manually install the binaries by building ROCm from source (not recommended for beginners).","Installing Arch Linux is not easy for a beginner, therefore look into Manjaro or EndeavourOS- they both use an Arch Linux base."]},{"i":"arch-linux-1","l":"Arch Linux","p":["Install GPU drivers by following the official guide.","Install an AUR Helper. We'll use paru for this guide:","Install ROCm:"]},{"i":"not-using-arch-linux","l":"Not using Arch Linux?","p":["AMD has a comprehensive guide for installing ROCm. Please refer to their installation guide."]}],[{"l":"KoboldAI","p":["KoboldAI is a browser-based front-end for AI-assisted writing and chatting with multiple local and remote AI models.","KoboldAI also supports PygmalionAI - although most primarily use it to load Pygmalion, and then connect Kobold to Tavern. You can still use Kobold in its New UI with Chat mode."]},{"l":"Installation","p":["Please make sure you've read the Overview and Setting up your GPU pages first."]},{"l":"Windows","p":["Make sure you don't have a B: Drive.","Download KoboldAI and run the .exe file:","KoboldAI","When you reach this part, type 2 and press enter.","Search for KoboldAI in the Start Menu and launch it.","Don't launch KoboldAI as Administrator!"]},{"l":"Linux"},{"l":"Requirements","p":["git"]},{"i":"installation-1","l":"Installation","p":["Clone the repo:","Launch KoboldAI"]},{"l":"Using KoboldAI","p":["Once you've launched KoboldAI, click on AI on the top-left corner of the screen, then click on Chat Models and select PygmalionAI/pygmalion-6b. You can then click on Load.","Loading 28/28 layers on GPU needs around 16GB VRAM. You can assign fewer layers based on how much VRAM your GPU has access to. Do not put any layers on Disk!"]}],[{"l":"KoboldAI 4bit","p":["KoboldAI is a browser-based front-end for AI-assisted writing and chatting with multiple local and remote AI models.","This guide is for users with less than 10GB of VRAM. Pygmalion 6B with 4bit quantization can run on GPUs with 6GB of VRAM and above.","Use the Table of Contents to navigate."]},{"l":"Requirements","p":["Please make sure you :","Have read the Overview and Setting up your GPU pages first.","Have Git installed, tutorial here.","Don't have a B: Drive (Windows only)"]},{"l":"Installation"},{"l":"Windows","p":["The Windows Guide was written with the help of Peepy (lunarlemon#4643)."]},{"i":"installing-0cc4ms-koboldai-fork","l":"Installing 0cc4m's KoboldAI fork","p":["After that, The command prompt will take a few minutes to install all the Kobold requirements and create a umamba virtual environment. This environment will keep all the dependencies safely secured in one folder to prevent conflicts with external packages.","After you've done that, congrats! You're almost there. You need to download the model now.","First create a folder where you want it to be.","First you want to clone the Kobold-4bit repository:","Go inside that folder and open a PowerShell by holding down the Shift key while right-clicking. From the context menu, select the option to Open PowerShell Window here.","Go into your KoboldAI folder and search for the install_requirements.bat file. Make a right click on it and choose Run as administrator.","However, you want to change it to : http://localhost:5000/new_ui","It should open a tab on your browser with this url http://localhost:5000","It shouldn't take more than a few seconds to clone it.","Make sure you've read the git installation guide.","Now that you have opened PowerShell, you need to run a few commands. (Copy paste the command and press enter).","Now, go back into your Kobold folder again, and open the play.bat file.","Once in the new UI, go to the \"Interface\" tab and turn on Experimental UI option.","This fork require a manual installation, you can easily do it by following the instructions below.","This is how your PowerShell look like so far:","This is what allows for 4bit (or on Linux, also 8bit!)","When running the install_requirements.bat command, it will first ask you which way you want to install it. The choice is up to you but we recommend the 1. Temporary Drive Letter(Press 1 and enter).","When the end of the prompt look like this, you can press any key to close it.","You can now close this PowerShell."]},{"l":"Downloading the model","p":["In your Kobold folder, navigate to the models folder.","Right click on the models folder and select \"Open in Terminal\". You may download a model now using git clone commands. There are two models to choose from. If you run","it will download the main Pygmalion version, V3.","As an alternative, Pygmalion Version 8 Part 4 is also available for download. In comparison to V3, V8 was fine tuned on a larger dataset which according to user feedback improved coherency and general knowledge of the model at the cost of being a little less inclined to engage in NSFW roleplay. To download it, run","instead.","Your preferred model should be downloaded in your models folder. The following steps are identical no matter which model you've downloaded.","The current GPTQ implementation on Kobold needs your model name to be either 4bit.pt/ 4bit.safetensors or 4bit-128g.pt/ 4bit-128g.safetensors. Please rename them appropriately:","At that point, all the installation part is done."]},{"l":"Loading the model","p":["On the Kobold URL http://localhost:5000/new_ui, click on the home tab, then on Load Model","On the list, pick the Load a model from it's directory option.","Pick the model we just downloaded, MAKE SURE THE 4 BIT MODE IS ON, then click on load !","The model will now be loaded and be ready to be use, if you want to use it with SillyTavern, please go here."]},{"l":"Linux","p":["With Linux, you should be able to get both 8bit and 4bit support using Occam's fork. For 8bit, you will need the original model. For 4-bit, you can download any of the GPTQ quantized models."]},{"i":"requirements-1","l":"Requirements","p":["git","python==3.10.9","aria2","cmake","make","gcc"]},{"i":"installation-1","l":"Installation","p":["Clone Occam's fork of KoboldAI","Install the Kobold requirements","This is a 2.5GB download.","Navigate to your Kobold folder and right click on the models folder. Choose Actions -> Open Terminal Here. As the steps for downloading a model are the same for Windows, you may refer to this part of the guide: Downloading the model.","The model needs to be named either 4bit.pt/ 4bit.safetensors or 4bit-128g.pt/ 4bit-128g.safetensors to work. Make sure you properly rename it.","Start KoboldAI by running ./play.sh.","Navigate to the NewUI (you can simply add new_ui at the end of the URL) and activate the Experimental UI.","Click on Load Model on the left-side menu and choose the following option:","On the list, pick the Load a model from it's directory option.","Pick the model we just downloaded, MAKE SURE THE 4 BIT MODE IS ON, then click on load !","You can now run Pygmalion 6B on your low VRAM GPU. Rejoice."]}],[{"l":"TextGen WebUI","p":["Oobabooga's Text Generation WebUI is a gradio frontend for running large language models.","Unlike KoboldAI, it can be used as a standalone front-end - you can still connect to SillyTavern though."]},{"l":"Automatic Installation","p":["Please make sure you've read the Overview and Setting up your GPU pages first."]},{"l":"Windows","p":["Oobabooga has generously created a one-click installation script for Text-Gen WebUI. You can download it here:","Text-Gen WebUI Windows Installer","Simply extract the one-click-installers-oobabooga-windows.zip file and click on install.bat. That will install everything for you. Once the installation is finished, open the start-webui.bat file.","If you want to, you can connect Oobabooga to SillyTavern."]},{"l":"Linux","p":["Oobabooga has created a one-click installer for Linux as well. Simply download the .zip file below, open a terminal instance in the extracted directory and run chmod +x install.sh and then ./install.sh.","Text-Gen WebUI Linux Installer","If you want to, you can connect Oobabooga to SillyTavern."]},{"l":"Manual Installation","p":["You can also manually install the WebUI. This method is recommended because it's more fun. The following guide is applicable to both Windows and Linux, though the primary audience is Linux users."]},{"l":"Requirements","p":["python","git","miniconda"]},{"l":"Installation steps","p":["Install miniconda:","Miniconda3 Windows Miniconda 3 Linux","For Windows, it's simply a matter of double-clicking the downloaded .exe file for installation. For linux, you'll need to make it an executable by running chmod +x Miniconda3-latest-Linux-x86_64.sh and then running the installer script with ./Miniconda3-latest-Linux-x86_64.sh in your terminal.","Create a conda environment for TextGen:","If you are, replace the third command with this: pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.2","Run the web server:","By default, oobabooga's TextGen will load the first model (in alphabetical order) placed in your models folder. You can specify which model to load with the --model argument, followed by your model name as it's named in the models folder. Here's an example of how you'd run Pygmalion-6b:","python server.py --model PygmalionAI/pygmalion-6b --cai-chat --auto-devices --no-stream","The PygmalionAI/ part is for if you want to download the model first. If you already have pygmalion-6b downloaded in your models folder, then simply do --model pygmalion-6b.","Please don't forget to pass the --load-in-8bit argument too if you have a low VRAM PC! --auto-devices should take care of the memory assignment if you have less 10GB VRAM.","You can view the full list of commands here.","If you want to, you can connect Oobabooga to SillyTavern."]}],[{"l":"SillyTavern","p":["SillyTavern is a user interface you can install on your computer (and Android phones) that allows you to interact with text generation AIs and chat/roleplay with the characters you or the community create.","SillyTavern is just a UI; you will need to connect it to a backend: KoboldAI or TextGen WebUI."]},{"l":"Installation","p":["Installing SillyTavern is simple. It supports the following operating systems:","Windows x64","Linux x64","MacOS (Darwin x64)","Android (aarch64)"]},{"l":"Windows Installation","p":["First, you will need to install Node.js.","Just open the installer, and click on Next, Next, etc.. (You don't have to change anything in the installer) Once Node.JS is installed, you can download Silly Tavern !","You'll need Github Desktop to clone the repository.","After installing GitHub Desktop, click on Clone a repository from the internet.... (Note that you do not need to create an account for this step)","On the menu, go into the URL tab, enter this url https://github.com/Cohee1207/SillyTavern and click clone.","You can change the path where SillyTavern is going to be downloaded.","To open SillyTavern, just go into the folder and double click on the start.bat file.","If everything is working, the CMD should look like this and a SillyTavern tab should be open in your browser.","You still have to connect it!"]},{"i":"linuxmacos-installation","l":"Linux/MacOS Installation"},{"l":"Requirements","p":["nodejs","git","Install nvm(NodeJS Version Manager) by running the following command in Terminal:","Run this to make nvm usable:","Install NodeJS by running the following command:"]},{"i":"installation-with-npx","l":"Installation (with npx)","p":["Run npx sillytavern@latest. You can re-open it anytime by running the command again."]},{"i":"installation-manual","l":"Installation (manual)","p":["Clone the repo","Run TavernAI"]},{"l":"Android Installation","p":["Install Termux.","Run the following commands inside Termux, in order:","Update repos/packages:","Install NodeJS and git:","Download Silly Tavern:","If you receive the above error, it means you've already downloaded SillyTavern before. In that case, you can simply update it. Change directories to SillyTavern by running cd SillyTavern, and then updating it by running git pull.","You can run SillyTavern again by simply opening Termux and entering the command above!"]},{"l":"Connect SillyTavern","p":["If you're running KoboldAI locally, all you need to paste in there is http://localhost:5000/api. If you're using Google Colab, copy your remote URL instead. If it ends with a # or new_ui, remove them and replace them with /api. If they don't, simply adding /api will suffice.","Please switch to Desktop Mode in your browser settings.","You can now start using SillyTavern! SillyTavern uses Character Cards, which are images (PNG and WEBP) that can be imported to Tavern as Characters. You can also find new characters in the unofficial Discord server."]}],[{"l":"Soft Prompts"},{"i":"what-are-soft-prompts-sp-and-do-i-need-them","l":"What are Soft Prompts (SP) and do I need them?","p":["A soft prompt is a file that modifies the style and behavior of the language model. It can create a bias towards the tone or the style of an author or series, and the model can then use this information to generate an output that fits your purposes for a specific bot(s). Even if they can help the model to internalize lore, facts or information, they are not suited for it. SPs do not re-train model and as such, no new knowledge is acquired. If you're interested in how softprompts work on a technical level, please refer to [this section]"]},{"l":"Using Soft Prompts"},{"l":"KoboldAI"},{"l":"Downloading and placing the SP","p":["First, download your SP of choice from either the Discord or this rentry a generous anon is maintaining.","For using a Soft Prompt, the method is slighlty different depending on the way you are running the model."]},{"l":"Locally","p":["If you're running KoboldAI locally on your PC (not using Google Colab), place the .zip file (don't extract!) inside the \\KoboldAI\\softprompts folder."]},{"l":"Google Colab","p":["Make sure to check the use_google_drive option and granting access to your Google Drive in the pop-up.","Then on the left of the screen click on the folder icon. Navigate through the files to find the \\MyDrive\\KoboldAI\\softprompts folder. Drag and drop your .zip file (don't extract!) inside."]},{"l":"Loading the Soft Prompt","p":["Once it's done, go into the Kobold UI, make sure the model is loaded (this is done automatically on Colab), and click on the Soft Prompt tab.","For Google Colab user, please click on the remote URL provide by the colab to reach that UI!","Then select your Soft Prompt and click on the load button.","If your Soft Prompt is loaded, the light bulb should be green!"]},{"i":"how-do-soft-prompts-work","l":"How do Soft Prompts work?","p":["Refer to this paper for a more detailed overview, and this wiki page for a more in-depth and human-readable explanation.","Soft prompting is an alternative to model fine-tuning that freezes the weights of a model and updates the parameters of a prompt to adapt the model to different downstream tasks. This results in a \"soft prompt\".","The advantage of soft prompting is that it allows the same model to be used for mutliple tasks by appending the appropriate prompts at inference time. This makes batching across different tasks easier. Soft prompts trained for a single model across multiple tasks will often be of the same token length. The vectors of values of the tokens can be considered as model parameters, and the model can be further trained by adjusting only the weights of these prompts.","Prompt tuning performs better with larger models, and using more than 20 tokens doesn't yield significant performance gains.","TL;DR, Soft prompts are learnable vectors that are concatenated to the input text and optimized end-to-end over a training dataset. Mixtures of soft prompts can be used to query LMs, allowing us to learn which prompts are most effective and how to ensemble them."]},{"i":"how-do-i-create-a-soft-prompt","l":"How do I create a Soft Prompt?","p":["Creating a softprompt for a a GPT-J model needs several times the computation power it takes to inference the model. KoboldAI has a Colab Notebook. As of now (2023-04-22), Colab's TPUs have a driver issue and so the Easy Tuner won't work. You can instead refer to this repo for GPU tuning - though you'll likely need a powerful GPU."]}],[{"l":"LoRA","p":["Low-Rank Adaptation (LoRA) is a paradigm of natural language processing (NLP) that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. Compared to GPT-175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirements by 3 times. Refer to the paper below:","LoRA: Low-Rank Adaptation of Large Language Models"]},{"l":"Low-Rank Adaptation","p":["LoRAs are preferred when the traditional fine-tuning methods prove to be too computationally expensive. As demonstrated by Edward Hu et al., training a LoRA is at least 3 times less resource intensive as a native fine-tune. This can be further reduced with INT4 LoRA tuning - made possible by GPTQ.","Some of the key advantages LoRAs possess are:","A pre-trained model can be shared and used to build many small LoRA modules for different downstream tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices A and B as demonstrated below, reducing the storage requirement and task-switching overhead significantly.","Figure 1","LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times (over 10 times if used in conjunction with GPTQ) when using adaptive optimizers since we don't need to calculate the gradients or maintain the optimizer states for most parameters. Instead we only optimize the injected, much smaller low-rank matrices.","The simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, instroducing no inference latency compared to a fully fine-tuned model, by construction.","LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning and prompt-tuning.","LoRAs were originally designed for LLMs, but they can be used for other objectives, such as diffusion models. The docs here will focus on the language modeling objective of LoRAs. We try to maximize the likelihood of a predicted word given the previous words in a sentence. Imagine we're given a GPT model \\Rho_\\Phi(y|x) which is parametrized by \\Phi. For example, \\Rho_\\Phi(y|x) can be a generic model based on the Transformers architecure. Now imagine we will need to adapt this large model to different downstream tasks, such as summarization, machine reading comprehension, and natural language to SQL (NL2SQL). We'll represent each downstream task by a training dataset of context-target pairs: \\large {Z=\\text{\\textbraceleft}(x_i,y_i)\\text{\\textbraceright}}_{i = 1,...N} where both x_i and y_i are sequences of tokens. For example, in summarization, x_i is the content of the article and y_i is its summary. During full fine-tuning, the model is initialized to pre-trained weights \\Phi_0 and updated to \\Phi_0 + \\Delta\\Phi by repeatedly following the gradient to maximize the conditional language modeling objective:","One of the main disadvantages for full fine-tuning is that for each downstream task, we learn a different set of parameters \\Delta\\Phi, whose dimensions |\\Delta\\Phi| equals |\\Phi_0|. If the model is large (such as GPT-3 with 175 Billion parameters (|\\Phi_0|)), storing and running independent instances of the model for different tasks would be extremely difficult.","LoRAs aim to solve this exact problem. The task-specific parameter increment \\Delta\\Phi = \\Delta\\Phi(\\Theta) is encoded by a much smaller-sized set of parameters \\Theta with |\\Theta| \\ll |\\Phi_0|. The task of finding \\Delta\\Phi then becomes optimizing over \\Theta:","LoRA uses a low-rank representation to encode \\Delta\\Phi that's both compute and memory efficient. When the model is GPT-3 175B, the number of trainable parameters |\\Theta| can be as small as 0.01% of |\\Theta_0|!"]},{"i":"how-does-it-work","l":"How does it work?","p":["A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers ususally have full-rank. When adapting a specific task, GPT models have a low \"instrisic dimension\" and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, Edward Hu et al. hypothesized that the updates to the weights also have a low \"instrinsic rank\" during adaptation. For a pre-trained weight matrix W_0 \\in \\R^{d x k}, we constrain its update by representing the latter with a low-rank decomposition W_0 + \\Delta W = W_0 + B A are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For h = W_0x, our modified forward pass yields:","Edward Hu et al. illustrate the reparametrization in Figure 1. They used a random Gaussian initialization for A and zero for B, so \\Delta W = B A is zero at the beginning of the training. We then scale \\Delta W_x by \\large{\\frac{\\alpha}{r}}, where \\alpha is a constant in r. When optimizing with Adam, tuning a is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set a to the first r we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary r. This method introduces no additional inference latency. When deployed in production, we can explicitly compute and store W = W_0 + B A and perform inference as usual. Note that both W_0 and B A are in \\R^{d x k}. When we need to switch to another downstream ask (switch LoRA adapters), we can recover W_0 by subtracting B A and then adding a different B' A', a quick operation with very little memory overhead."]},{"i":"what-are-the-benefits-of-lora","l":"What are the benefits of LoRA?","p":["The most significant benefit comes from the reduction in memory and storage usage. For a large model, such as GPT-J, trained with Adam, we reduce that VRAM usage by up to 2/3 if r \\ll d_{model} as we do not need to store the optimizer states for the frozen params. On GPT-3 175B, we can reduce the VRAM consumption during training from 1.2TB to 350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000x (from 350GB to 35MB). This allows us to train with significantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for creation of many customized models that can be swapped in and out on the fly.","LoRA also has its limitations. For example, it's not straightforward to batch inputs to different tasks with different A and B in a single forward pass, if one chooses to absorb A and B into W to eliminate additional latency. Though it's possible to not merge the weights and dynamically choose the LoRA modules to use for samples in a batch for scenarios where latency is not very important."]},{"i":"how-do-i-train-a-lora","l":"How do I train a LoRA?","p":["The xTuring repository contains code for both LoRA and full fine-tuning of LLMs such as LLaMA, GPT-J, and more. The current GPT-J (Pygmalion 6B) training code is only compatible with Alpaca and GPT4All dataset formats, so it will likely be useless for most users. There's currently work being done on creating INT4 LoRA training code for GPT-J, so please be patient and keep an eye out for any updates here."]}],[{"l":"Overview","p":["Due to recent strides in the AI field, it is now possible to run Large Language Models entirely on CPU with surprisingly decent speeds. One such library is GGML. This makes it possible to run Pygmalion 6B even on mobile phones! (Provided you have at least 8GB of RAM.) In this section, you'll find guides for CPU inferencing, applicable to both PCs (irregardless of your Operating System) and Mobile Phones (Android only for now).","The system requirement for Pygmalion C++ is about 4-6GB of RAM. This isn't VRAM, and you don't even need to have a GPU at all!","Pygmalion C++ will take a few minutes to process your input tokens when you first load a character. This is normal and won't be repeated every time you type in a prompt for your bot.","Please continue to the next section."]}],[{"i":"pygmalion-c","l":"Pygmalion C++","p":["Initially, the only way to run Pygmalion C++ was through this repo: AlpinDale/pygmalion.cpp. However, thanks to the efforts of concedo from the KoboldAI team, we now have an easy-to-run executable for windows, and a compilable UI for Linux/MacOS/Android users."]},{"l":"The 4-bit quantized 6B model","p":["Pygmalion-6B 4bit (concedo)","If the model above doesn't work for any reason, you can download the old one instead:","Pygmalion-6B 4bit (alpindale)"]},{"l":"Windows Guide","p":["Running on Windows is exceedingly simple. You can download the .exe file from here, and the Pygmalion 6B model linked above.","Once you have both files downloaded, all you need to do is drag the pygmalion-6b-v3-q4_0.bin file and drop it into koboldcpp.exe file. This will load the model and start a Kobold instance in localhost:5001 on your browser. You can also simply double-click on the .exe file."]},{"l":"MacOS Guide","p":["The ggml library is primarily designed for Apple Silicon, and as such, it runs the best on native Mac. There are no pre-compiled binaries available for MacOS, but compiling it yourself should be very simple."]},{"i":"requirements","l":"Requirements:","p":["Python","C toolchain"]},{"i":"install-the-requirements","l":"Install the requirements:","p":["Open a terminal.","Paste this command and press enter to execute it:","Install Python from here."]},{"i":"installation","l":"Installation:","p":["Download the latest release file from here. You're looking for the Source code (zip) file.","Extract it anywhere you want.","Open a Terminal inside the extracted folder.","Type in make and let it compile the program.","Once it's finished, download the model and place it inside the koboldcpp folder.","Run the model with:","Open your browser and navigate to http://127.0.0.1:5001.","You're done!"]},{"l":"macOS Quickstart","p":["If you want to launch it again later, all you need to do is open a terminal inside the koboldcpp folder, and run:","Make sure you bookmark http://localhost:5001 so you can easily open the Koboldcpp interface."]},{"l":"Linux Guide"},{"i":"requirements-1","l":"Requirements:","p":["python","gcc","python-pip","git","wget"]},{"i":"installation-1","l":"Installation:","p":["Run these commands in order:","Open your browser and navigate to localhost:5001. You're all set."]},{"l":"Linux Quickstart","p":["If you want to launch it again later, all you need to do is open a terminal inside the koboldcpp folder, and running:","Make sure you bookmark http://localhost:5001 so you can easily open the Koboldcpp interface."]},{"l":"Android","p":["Your phone needs to have at least 8GB of RAM to run Pygmalion C++. Modern phones use over 4GB of RAM by themselves, so you won't be left with much room for Pygmalion C++ if you don't have at least 8GB."]},{"i":"requirements-2","l":"Requirements:","p":["Termux","Install Termux from F-Droid."]},{"i":"installation-2","l":"Installation:","p":["Open Termux.","Type in the following commands one by one and press enter to run them:","pkg update pkg upgrade","pkg install python clang python-pip git openssl","pip install psutil","git clone https://github.com/LostRuins/koboldcpp cd koboldcpp","make","wget https://huggingface.co/concedo/pygmalion-6bv3-ggml-ggjt/resolve/main/pygmalion-6b-v3-ggml-ggjt-q4_0.bin","python koboldcpp.py pygmalion-6b-v3-ggml-ggjt-q4_0.bin","Switch to your browser (don't close Termux) and go to localhost:5001.","You're done!"]},{"l":"Android Quickstart","p":["If you want to launch it again, open Termux and make sure you're in the koboldcpp folder. You can confirm this by checking whether the bash prompt is ~ $ or ~/koboldcpp $. If it's the former, run cd koboldcpp. If it's the latter, continue.","Type this in to run koboldcpp again:","Now open http://localhost:5001 on your browser."]},{"l":"Connecting to Tavern","p":["You can connect Pygmalion C++ to Tavern the same way you would the regular KoboldAI. There's a guide included here."]}],[{"i":"whats-git-and-do-i-need-it","l":"What's Git and do I need it?","p":["Git is a popular version control system used by software developers to manage their code. It allows developers to track changes to their code over time, collaborate with others, and revert back to previous versions if needed. Git gives you access to useful commands, such as git clone. You can use it to create a local copy of a remote repository, including all its files and history, on your local machine. The most useful command for an end-user is git pull, which allows you to update a cloned repository easily without having to re-download the zip file and merging it into your cloned folder.","Many of our tutorials use Git, so you should consider installing it by following the instructions below.","Windows x64","MacOS (Darwin x64)","Linux x64"]},{"l":"Windows","p":["Installation is very simple. Open the Start Menu and search for PowerShell. Right-click on it and Open as Administrator. Once the blue PowerShell window pops up, simply type these two commands in order and press enter to execute them:","Install git:","Install git lfs","Close PowerShell."]},{"l":"macOS","p":["To make things easier, we will use \"Homebrew\", a commandline software installer for macOS.","First you will need to open the Terminal application on your Mac. You can find it in the \"Utilities\" folder within the \"Applications\" folder.","Then run this command in the terminal (Copy paste it in the terminal and press enter):","Once it's done, simply run brew install git","At this point, you've installed Git on your Apple Silicon. You can now verify by running this command in your terminal: $ Git --version"]},{"l":"Linux","p":["I shouldn't have to explain this to you, Linux user. git is on every package manager. However, I like extensive documentation, so I will list instructions for all distros.","If you're unsure what your distro is, run uname -v | awk '{print $4}'.","Install git and git-lfs:"]}],[{"l":"Settings and Parameters","p":["You'll find a list of all settings in KoboldAI, Oobabooga's Text Generation WebUI, and TavernAI. Please use the Table of Contents to navigate through this page.","Currently you'll only find a list of settings in KoboldAI. The other UIs will be added soon but they're more or less the same."]},{"l":"KoboldAI"},{"l":"Output Length","p":["Output Length is a setting used to control the length of the output generated by the model. The output length setting will determine how long the model's reply will be based on the value you set.","For example, if you set the output length to 200, the model will generate a piece of text that is approximately 100 tokens long, depending on the specifics of the model and the input data.","The Output Length setting can be useful for controlling the amount of output generated by the model and for ensuring that the generated text is of a consistent length. A higher value will take longer to generate."]},{"l":"Temperature","p":["Temperature determines the randomness of sampling. A higher value can increase creativity, but can make the output less meaningful and coherent. Lower values will make the output more predictable, but it may become more repetitive."]},{"l":"Top P Sampling","p":["Top_P is a way of generating text with an AI model that helps to make the output more diverse and interesting. When using Top_P, you select the most likely words to come next based on their probability of occurring, but only up to a certain cumulative probability threshold. This means that instead of always selecting the most likely word, the language model can choose from a wider range of words that are still reasonably likely to occur.","For example, if you set Top_P to 0.5, the language model will only consider words that make up the top 50% of the probability distribution for the next word, rather than just the most likely word.","You can can use Top_P to make the model output more varied and creative. By not always selecting the most likely word, the language model can surprise you with unexpected but still plausible words, creating more interesting and engaging text."]},{"l":"Top K Sampling","p":["When using \"top k\", you select the top k most likely words to come next based on their probability of occurring, where k is a fixed number that you specify.","For example, if you set Top_K to 5, the language model will only consider the five most likely words to come next, rather than all the words in its vocabulary.","You can use Top_K to control the amount of diversity in the model output. By limiting the selection to a small number of the most likely words, the language model can produce more predictable and conservative text. However, by increasing k, you can allow for more unexpected and creative choices in the generated text.","Overall, Top_K can be useful for balancing the tradeoff between predictability and creativity in language model output."]},{"l":"Combining Top K with Top P","p":["Combining Top_K and Top_P is a technique called nucleus sampling or Top_P sampling with Top_K cutoff. This technique allows you to use both methods together to generate even more diverse and interesting text.","Here's how it works: first, the model uses Top_p to select the most likely words up to a certain cumulative probability threshold, as explained in the Top_P page","Then, it applies a Top_K cutoff to this set of words, selecting only the top k most likely words from the Top_P selection.","By using both techniques together, you can generate text that is both diverse and controlled. The Top_P selection ensures that the language model has the freedom to choose from a range of plausible words, while the Top_K cutoff limits the number of choices to a manageable set of highly likely words.","The specific values of Top_P and Top_K can be adjusted to achieve different levels of diversity and control in the generated text. For example, a higher Top_P value will allow for more diverse choices, while a lower Top_K value will provide more control over the output. It's important to experiment with different settings to find the right balance for your specific use case."]},{"l":"Tail Free Sampling","p":["Tail Free Sampling is a setting used to improve the consistency of the generated output. When generating new text, the model can use Tail Free Sampling to select the next word, but with less emphasis on the rare or extreme values.","The slider is a tool that allows you to adjust the amount of Tail Free Sampling used by the model. When you pick a higher value, the model will be more likely to avoid selecting the rare or extreme values, which can help to increase the consistency of the generated output. This is because the model will be working from the bottom of the probability distribution, selecting more common and likely values and trimming the lowest probability tokens."]},{"l":"Typical Sampling","p":["Typical Sampling is a setting used to control how the model selects the next token or word in the generated output. With typical sampling, the model selects the next token based on its probability distribution, meaning that more probable tokens are more likely to be selected.","For example, if the model is generating a sentence and the next probable word is \"the\", the model is more likely to select \"the\" as the next word with typical sampling. This can help to ensure that the generated output is fluent and coherent, as the model is more likely to select words that make sense in the given context."]},{"l":"Top A Sampling","p":["Top A Sampling is a way to pick the next word in a sentence based on how important it is in the context. We use a \"leader\" number between 0 and 1 to help us decide which words are most important. Top-A considers the probability of the most likely token, and sets a limit based on its percentage. After this, remaining tokens are compared to this limit. If their probability is too low, they are removed from the pool. Increasing A results in a stricter limit. Lowering A results in a looser limit.","This means that if the top token has a moderate likelihood of appearing, the pool of possibilities will be large. On the other hand, if the top token has a very high likelihood of appearing, then the pool will be 1-3 tokens at most. This ensures that structure remains solid, and focuses creative output in areas where it is actually wanted."]},{"l":"Repetition Penalty","p":["Repetition Penalty is a setting used to control how often the model repeats certain words or phrases in the generated output. When the repetition penalty is set to a higher value, the model is less likely to repeat the same word or phrase multiple times in the output."]},{"l":"Repetition Penalty Range","p":["If set higher than 0, repetition penality only applies to the last few tokens of the story rather than to the entire story. The slider controls the amount of tokens at the end of your story to apply it to."]},{"l":"Repetition Penalty Slope","p":["If both this setting and Repetition Penality Range are set higher than 0, the repetition penality will be apply more strongly on tokens that are close to the end of the story. High values will result in the repetition penalty difference between the start and the end of your story being more apparent."]},{"l":"Alt Rep Pen","p":["Alt Rep Pen applies Repetition Penalty as a logarithmic modifier rather than a linear modifier. This means that the penalty will be smaller for the first few repetitions and will increase more slowly as the number of repetitions increases.","In simpler terms, the more your model repeats a token, the more severe the penalty becomes."]},{"l":"Context Tokens","p":["Number of tokens to submit to the model for sampling/that are used to generate the output. Make sure this is higher than \"Output Length\". Higher values increase the memory but it also increases the usage of VRAM/RAM."]},{"l":"Gens Per Action","p":["The number of outputs the model will generate at once. Use this setting if your responses are too short; it will make the bot generate twice in order. Increases VRAM/RAM usage."]},{"i":"wi-depth-world-info","l":"WI Depth (World Info)","p":["Number of actions (Input/Output) from the end of the story to scan for World Info keys.","Excerpt from the KoboldAI wiki:","World Info is where you can flesh out the details of your wider world. The engine will help conserve tokens in your context by only inserting World Info entries when their keywords are mentioned in the actual story text. However, they are inserted toward the top, after the Memory but before the actual story text, so they have a moderate effect on what the AI generates.","World Info entries are like an encyclopedia entry, providing a succinct overview of the most relevant information about whatever topic - characters, species, places, items, etc...","The AI doesn't see the key word or title of the World Info entry \"behind the scenes\", so the actual text should be an entirely self-contained description. For example, if you have a World Info entry titled \"Director Abrams\", the entry should say \"Director Abrams is the executive and governor of the colony\" rather than just \"the executive and governor of the colony\", because the AI will only see the entry.","Use World Info entries that cross reference each other appropriately to create a more dynamic and interactive world. If your \"combat android\" World Info entry mentions that they carry plasma rifles, then create a short \"plasma rifle\" entry separately. If you create a \"Admin Tower\" entry that mentions containing a command center, computer core, and offices, create distinct \"command center\", \"computer core\", and \"offices\" entries that mention being located in the Admin Tower.","It helps to use many key words that might pull in a World Info entry whenever it might be appropriate, rather than just its proper name(s).","If you have a rich tapestry of interconnected World Info entries with many relevant keywords, you may find that multiple World Info entries are getting pulled into the context at any given time. So it's good to keep them fairly short (50 tokens for minor stuff, 100 tokens for significant characters, no more than 150 for major keystones of the setting)."]},{"l":"Prefilled Memory","p":["If you create a \"Random Story\" by clicking on the \"New Game\" button with this setting enabled, the memory of the current story will be loaded into the memory of the new Random Story."]},{"l":"Chat Mode","p":["This mode optimizes KoboldAI for chatting/using conversational models. Use this mode for Pygmalion or other chatbot models."]},{"l":"Adventure Mode","p":["Turn this on if you are playing a \"Choose Your Adventure\" model. Does not apply to Pygmalion."]},{"i":"dynamic-wi-scan-world-info","l":"Dynamic WI Scan (World Info)","p":["Scans the AI's output for World Info keys as it is generating the output. If you disable this, KoboldAI will only scan the output after the bot has finished generating the text."]},{"l":"Token Probabilities","p":["Add a context menu beside the output showing what other words were considered as it was generated."]},{"l":"Token Streaming","p":["Shows tokens as they are generated. Creates a similar effect to when other models like ChatGPT or Character.AI display their output in real-time."]}]]