[[{"i":"what-is-pygmalionai","l":"What is PygmalionAI?","p":["PygmalionAI is an open-source large language model ( LLM) based on EleutherAI's GPT-J 6B.","In simple terms, Pygmalion is an AI fine-tuned for chatting and roleplaying purposes. The current actively supported Pygmalion AI model is the 6B variant, a model with 6 billion parameters."]},{"l":"Features","p":["Unrestricted: No measures have been put in place to restrict the output.","Low VRAM requirement: With only 16GB (or less) VRAM required, Pygmalion offers better chat capability than much larger language models with relatively minimal resources.","Fine-tuned for RP: Our curated dataset of high-quality roleplaying data ensures that your bot will be the optimal RP partner.","Free and Open-Source: Both the model weights and the code used to train it are completely open-source, and you can modify/re-distribute it for whatever purpose you want.*","Regularly updated: Pygmalion is regularly updated with new data to improve the model even further.","* PygmalionAI uses the CreativeML Open RAIL-M license, which is also used by Stable Diffusion."]},{"i":"how-do-i-use-pygmalion","l":"How do I use Pygmalion?","p":["Language models, including Pygmalion, generally run on GPUs since they need access to fast memory and massive processing power in order to output coherent text at an acceptable speed. Pygmalion is no different. You need a powerful GPU to run the model.","If you don't have a powerful enough GPU, there are luckily alternative solutions. Google's Colaboratory offers free GPUs for all users - though for a limited time. We have ready-made notebooks that you can run to access Pygmalion. You can also use Colab on your mobile phone.","These docs will attempt to explain all the possible methods for running the AI, and in a language that is easy to understand. We will start with Google Colab, as it's generally the easiest method."]}],[{"l":"Frequently Asked Questions"},{"i":"how-do-i-use-the-latest-dev-v8-version","l":"How do I use the latest dev (v8) version?","p":["If you're running on Google Colab, all notebooks have an option for downloading the dev(beta) model. Look for either Pygmalion 6B Dev or Pygmalion 6B Experimental.","If you're running locally, make sure git is installed and run the following command inside your KoboldAI/models or text-generation-webui/models folder:","If it says git lfs was not recognized as a command, please install git-lfs with your package manager (Linux) or download the installer for Windows."]},{"i":"my-character-has-terrible-memory","l":"My character has terrible memory!","p":["This could be due to the limited context size. Pygmalion 6B has a maximum context size of 2048 tokens. This includes your character description, example messages, and all your chatlogs. The description and examples are placed at the top of the context memory - all your subsequent chat logs are placed beneath them. This means that if your character description + examples chats are 400 tokens, you'll only have 1648 tokens left for your messages. The bot will forget everything past those 1648 tokens.","Character editors/creators such as TavernAI's will give you a token count for your character, but you can also use OpenAI's tokenizer service.","OpenAI Tokenizer"]},{"i":"my-characters-responses-are-too-short","l":"My character's responses are too short!","p":["The character will mimic the example chats and greeting message's style. Keep these in mind if you want to create a verbose character:","Be descriptive and verbose in your greeting message and example chats.","Be descriptive and verbose in responses by the user (you!).","Re-generate the response until you see a satisfactory one.","Manually edit the character's response to make it more verbose."]},{"i":"my-character-is-generating-nonsense","l":"My character is generating nonsense!","p":["Your temperature or repetition penalty settings are likely too high. The recommended range for temperature (for chatbots) is between 0.5 to 0.9 and the ideal range for repetition penalty is between 1.1 to 1.2."]},{"i":"my-character-is-not-responding-anymore","l":"My character is not responding anymore!","p":["You have probably run out of GPU memory (also known as VRAM). If you are using the free Google Colab plan, the GPU provided by Google can't handle the maximum context size (2048 tokens). If you have 16GB of VRAM, please tweak the context size to 1400-1600 (proportionally lower according to your VRAM size)."]},{"i":"for-text-gen-oobabooga-you-need-to-check-the-load_in_8bit-option-under-the-3rd-cell-before-running-it","l":"For Text-Gen (Oobabooga) you need to check the load_in_8bit option under the 3rd cell before running it."},{"i":"for-tavernai-running-the-model-in-8bit-is-not-available-the-only-solution-is-to-slide-down-the-context-size-to-around-1400-or-1600-in-the-settings-tab","l":"For TavernAI, running the model in 8bit is not available. The only solution is to slide down the context size to around 1400 (or 1600) in the settings tab."},{"i":"do-you-keepshare-any-of-my-datachat","l":"Do you keep/share any of my data/chat?","p":["The Pygmalion project team does not collect any data other than the chat logs you explicitly consent to donating for the training dataset. This project is done out of passion, and no one has any intention of collecting, analyzing, or selling any of your data. The Colab notebook automatically passes the --quiet argument in KoboldAI, which means your/the bot's responses aren't printed out for Google to see."]},{"i":"what-are-softprompts-do-i-need-them","l":"What are Softprompts? Do I need them?","p":["A soft prompt is a way to modify the style and behavior of your AI.","They're made by taking a (possibly) huge amount of information and compressing them into a small number of tokens, then feeding it into the embedding layer of the model. Even if people tend to use it that way, softprompt are not made to add context, lore, etc to the story. But yes, it might work for that if the dataset use to make the softprompt was big enough and made use of good quality data."]},{"i":"how-do-i-use-softprompts","l":"How do I use Softprompts?","p":["Download your SP of choice from either the Discord or this Rentry a generous anon is maintaining.","If you're running locally: place the .zip file (don't extract!) inside the softprompts folder - both Oobabooga and KoboldAI have the same folder.","If you're using Google Colab, open your Google Drive and find the KoboldAI/softprompts folder and upload your .zip file there.","Make sure Pygmalion is loaded, then an option for Softprompts should appear."]},{"i":"which-ui-should-i-use-theres-so-many-of-them","l":"Which UI should I use? There's so many of them.","p":["It's up to you, every UI tends to add the same features others as well as try to be compatible with characters from other UIs as well. If you want to focus on story generation, we recommend using KoboldAI. For chat purposes, TavernAI has the best user interface, followed by Oobabooga. There are other TavernAI alternatives (e.g. miku.gg), but it's up to you in the end. Please refer to their Colab Pages for a preview of the UI.","--"]},{"i":"im-using-google-colab-how-much-time-do-i-have-to-use-their-gpus","l":"I'm using Google Colab, how much time do I have to use their GPUs?","p":["The amount of time you can use when using the free plan for Google Colab highly depends on the traffic Colab recieves and your usage patterns. Google says that you can use a Colab Notebook for at most 12 hours if you don't have Compute Units. Pygmalion users tend to say it's less - usually anywhere from two to six hours."]},{"i":"how-to-avoid-reaching-the-gpu-quota","l":"How to avoid reaching the GPU quota?","p":["When you're done using Pygmalion, please terminate your Colab session! You'll waste your quota otherwise, and might find yourself unable to connect to a GPU backend the next time you login."]},{"i":"ive-out-of-gpu-time-what-do-i-do-now","l":"I've out of GPU time, what do I do now?","p":["If you've run out of GPU time, you can do nothing but wait for your quota to reset. Alternatively, you can purchase Colab Pro to increase your daily/weekly quota."]},{"i":"whats-a-compute-unit-cu-should-i-buy-one","l":"What's a Compute Unit (CU)? Should I buy one?","p":["A CU is a unit that you can buy to be sure to be granted access to a powerful GPU. Pricing for Compute Engine is based on per-second usage of the machine types, persistent disks, and other resources that you select for your virtual machines. So it's up to you."]},{"i":"ive-run-out-of-quota-but-i-cant-buy-a-compute-unit-what-do-i-do","l":"I've run out of quota, but I can't buy a Compute Unit! What do I do?","p":["In that case, you can use Kobold Horde. Generous users are donating their GPU power to the Horde so that people can use them to run language models. Pygmalion is a popular model, so you'll always find people hosting it. Keep in mind that there are more Horde users than workers, so you might have to wait a bit longer for responses."]}],[{"l":"Colab Notebooks","p":["Running Pygmalion - or any large language model - requires a significant amount of VRAM. The current absolute minimum requirement is 10GB for a comfortable experience. Anything lower, and you would either be unable to use the model, or generations will be too slow to be enjoyable.","Google has generously offered free GPUs for in their Colaboratory. The Colab's free plan offers an NVIDIA T4 GPU (16 GB of VRAM), which is more than enough to run Pygmalion-6B. A TPU v2-8 is also available in the free plan, but availibility is low due to high demand."]},{"l":"Notebooks","p":["There are currently two backends to run the Pygmalion 6B model - KoboldAI and Text-Generation-WebUI by oobabooga.","TavernAI is a frontend that connects to KoboldAI and essentially facilitates a connection to Pygmalion via Kobold while offering a pleasant looking User Interface. TavernAI by itself doesn't run Pygmalion - therefore you do not need a powerful PC to run TavernAI. You cannot use TavernAI with oobabooga's Text-Gen-WebUI.","If you're on Mobile, use either oobabooga's TextGen or TavernAI.","\uD83D\uDDA5️ Click here.","\uD83D\uDDA5️/\uD83D\uDCF1 Click here.","When you're done using Pygmalion, please terminate your Colab session! You'll waste your quota otherwise, and might find yourself unable to connect to a GPU backend the next time you login."]}],[{"l":"KoboldAI","p":["Each Colab notebook will greet you with a guide. For the sake of simplicity, visual guides will be included below. You can find the notebook here.","Make sure you're signed in to your Google Account before you attempt running any of the notebooks.","Scroll down and select the highlighted options, and then run the cell:","You might be warned that the notebook is not authored by Google - you can ignore it and click on Run anyway.","You'll be prompted to give the notebook access to your Google Drive files. Click on Connect to Google Drive. This will open a pop-up browser window where you'll give Colab access to your Google Account. Select the account you wish to use, then scroll down and click on \"Allow\".","Wait for the notebook to finish running. This can take up to 5 or 10 minutes.","Once it's loaded, two URLs will appear. If you wish to use KoboldAI with Tavern, please copy the first URL. If you wish to use Kobold by itself, please click on the second URL.","You can start using Pygmalion if you clicked on the second link. If you want to use it with Tavern, please refer to the TavernAI guide included here."]}],[{"l":"Text Generation WebUI","p":["activate_character_bias: Lets you add a snippet of text that'll be put before every reply your bot generates - but keeps it hidden from you. In simple terms, if you add *yells* in the character bias section, your character will yell every line.","activate_sending_pictures: Allows you to send pictures to your bots.","activate_silero_text_to_speech: Use this option if you want TTS for your bots.","cai_chat: Will mimic the appearance of Character.AI's user interface.","chat_language: Pygmalion is a primarily English-only model - this feature simply runs yours and your bot's responses through a translation service.","Each Colab notebook will greet you with a guide. For the sake of simplicity, visual guides will be included below. You can find the notebook here.","If you're on mobile, run this cell and keep the audio file playing:","load_in_8bit: Pygmalion 6B's weights are normally stored in 16-bit floating point precision. However, this option will load them in 8-bit integer precision instead, cutting down on the computational power required to run the model. Use this so you can increase context size to the maximum.","Make sure you're signed in to your Google Account before you attempt running any of the notebooks.","model: Use this option to select which version of the Pygmalion model you want - original is the first version of the model, main is the current stable release (v3), dev is the latest beta release.","Once the model is loaded, you can find your URL here:","Run the second cell, and make sure the save_logs_to_google_drive option is checked. If you don't check this, your characters and chatlogs won't persist across sessions on the same account:","Scroll down to the third cell and finally run Pygmalion:","text_streaming: The generated text by the bot is displayed in real-time instead of waiting for the whole message to finish generating before showing it to you.","Wait for the cell to finish loading Pygmalion. This cell will not stop running until you terminate your Colab session or manually stop it.","Wait for the cell to finish running. You can find out whether it's finished by checking if the cell button is still spinning:","Wait for the checkpoint shards to be loaded. This should take a few minutes at most.","You can now start chatting with any bot you want. You can find bots in our unofficial Discord Server.","You might be warned that the notebook is not authored by Google - you can ignore it and click on Run anyway.","You'll be prompted to give the notebook access to your Google Drive files. Click on Connect to Google Drive. This will open a pop-up browser window where you'll give Colab access to your Google Account. Select the account you wish to use, then scroll down and click on \"Allow\"."]}],[{"l":"TavernAI","p":["Each Colab notebook will greet you with a guide. For the sake of simplicity, visual guides will be included below. You can find the notebook here.","Make sure you're signed in to your Google Account before you attempt running any of the notebooks.","If you're on mobile, run this cell and keep the audio file playing:","You might be warned that the notebook is not authored by Google - you can ignore it and click on Run anyway.","Scroll down and run this cell to launch Pygmalion with Tavern:","Choose Pygmalion Dev in the Model drop-down menu if you want to try the latest beta release of the model.","You'll be prompted to give the notebook access to your Google Drive files. Click on Connect to Google Drive. This will open a pop-up browser window where you'll give Colab access to your Google Account. Select the account you wish to use, then scroll down and click on \"Allow\".","Wait around 5 minutes for TavernAI to finish loading. You can find the URL here at the end:","You can now start chatting with any character you wish! TavernAI offers community-created characters, but you can also add your own by clicking on the burger menu to the top-right corner of the screen, and then the +Import button in the Characters menu. Find more characters in the unofficial Discord server."]}],[{"l":"Overview","p":["16GB","20GB","24GB","AMD","Consumer-grade (Gaming) GPUs:","GPU","If you don't have any of these cards, but still have over 8GB of VRAM, chances are you can run the model just fine. You just have to make sure your card supports CUDA (NVIDIA) or ROCm (AMD). We'll get to that later.","Manufacturer","NVIDIA","Please continue to the GPU set-up section. Setting up your GPU","PygmalionAI is a large language model, and as the name might imply, it needs a lot of computation power to run it. You will need a minimum of 16GB VRAM to run the model - without any optimizations, that is. Here's a list of all the GPUs that would work without any tweaks, out-of-the-box:","Radeon RX 6800","Radeon RX 6800 XT","Radeon RX 6900 XT","Radeon RX 6950 XT","Radeon RX 7900 XT","Radeon RX 7900 XTX","RTX 3090","RTX 3090 Ti","RTX 4080","RTX 4090","Titan RTX","VRAM"]}],[{"l":"Setting up your GPU","p":["Consumer-grade GPUs are designed for graphics processing and 3D rendering - they're not specifically made for language processing models. However, GPU vendors such as NVIDIA and AMD have released toolkits for enabling deep learning computation for their GPUs. Only a handful of AMD GPUs are supported, but the majority of NVIDIA cards can be set up easily.","Keep in mind that the most important factor for AI is VRAM (video memory), not the processing power. e.g. an RTX 2070 (8GB) is better than an RTX 4050 (6GB) when it comes to AI inference.","Please refer to the links below to check if your GPU is compatible:","-","If your GPU is supported, please proceed with the installation."]},{"l":"NVIDIA","p":["NVIDIA cards are mostly supported, though it's not recommended to use GPUs with less than 8GB of VRAM.","The setup process will be different for both Windows and Linux (MacOS currently not supported)."]},{"l":"Windows","p":["The official Game-Ready Drivers provided by NVIDIA will set up CUDA for you, and that's all you need to do!"]},{"l":"Linux","p":["CUDA installation is relatively easy for Linux (unless you're on Fedora). For this particular guide, we'll be covering two base distros - Arch Linux and Debian. More will be added later."]},{"l":"Arch Linux","p":["Install the NVIDIA drivers (if they aren't already installed):","If you're on the LTS Arch Linux Kernel, then please install nvidia-lts instead. You can find what Kernel you're using by running uname -r","Install CUDA and (optionally) cuDNN:","You're done! Verify installation by running nvcc -v."]},{"l":"Debian","p":["Install the NVIDIA drivers (if they aren't already installed):","Install CUDA:","You're done! Verify installation by running nvcc -v."]},{"l":"AMD","p":["ROCm is (currently) not supported by Windows. You cannot run any language model on Windows if you have an AMD card - this isn't a Pygmalion-only issue. You will need to either dual-boot Linux or switch to Linux entirely if you wish to run language models on your AMD GPU."]},{"i":"linux-1","l":"Linux","p":["The ROCm installation is only (relatively) easy on Arch Linux. For other distros, you will need to manually install the binaries by building ROCm from source (not recommended for beginners).","Installing Arch Linux is not easy for a beginner, therefore look into Manjaro or EndeavourOS- they both use an Arch Linux base."]},{"i":"arch-linux-1","l":"Arch Linux","p":["Install GPU drivers by following the official guide.","Install an AUR Helper. We'll use paru for this guide:","Install ROCm:"]},{"i":"not-using-arch-linux","l":"Not using Arch Linux?","p":["AMD has a comprehensive guide for installing ROCm. Please refer to their installation guide."]}],[{"l":"KoboldAI","p":["KoboldAI is a browser-based front-end for AI-assisted writing and chatting with multiple local and remote AI models.","KoboldAI also supports PygmalionAI - although most primarily use it to load Pygmalion, and then connect Kobold to Tavern. You can still use Kobold in its New UI with Chat mode."]},{"l":"Installation","p":["Please make sure you've read the Overview and Setting up your GPU pages first."]},{"l":"Windows","p":["Make sure you don't have a B: Drive.","Download KoboldAI and run the .exe file:","KoboldAI","When you reach the updater script, Pick option 2.","Search for KoboldAI in the Start Menu and launch it.","Don't launch KoboldAI as Administrator!"]},{"l":"Linux"},{"l":"Requirements","p":["git"]},{"i":"installation-1","l":"Installation","p":["Clone the repo:","Launch KoboldAI"]},{"l":"Using KoboldAI","p":["Once you've launched KoboldAI, click on AI on the top-left corner of the screen, then click on Chat Models and select PygmalionAI/pygmalion-6b. You can then click on Load.","Loading 28/28 layers on GPU needs around 16GB VRAM. You can assign fewer layers based on how much VRAM your GPU has access to. Do not put any layers on Disk!"]}],[{"l":"TextGen WebUI","p":["Oobabooga's Text Generation WebUI is a gradio frontend for running large language models.","Unlike KoboldAI, it can be used as a standalone front-end - more importantly, you cannot connect Oobabooga's Text Generation WebUI to Tavern."]},{"l":"Automatic Installation","p":["Please make sure you've read the Overview and Setting up your GPU pages first."]},{"l":"Windows","p":["Oobabooga has generously created a one-click installation script for Text-Gen WebUI. You can download it here:","Text-Gen WebUI Windows Installer","Simply extract the one-click-installers-oobabooga-windows.zip file and click on install.bat. That will install everything for you. Once the installation is finished, open the start-webui.bat file."]},{"l":"Linux","p":["Oobabooga has created a one-click installer for Linux as well. Simply download the .zip file below, open a terminal instance in the extracted directory and run chmod +x install.sh and then ./install.sh.","Text-Gen WebUI Linux Installer"]},{"l":"Manual Installation","p":["You can also manually install the WebUI. This method is recommended because it's more fun. The following guide is applicable to both Windows and Linux, though the primary audience is Linux users."]},{"l":"Requirements","p":["python","git","miniconda"]},{"l":"Installation steps","p":["Install miniconda:","Miniconda3 Windows Miniconda 3 Linux","For Windows, it's simply a matter of double-clicking the downloaded .exe file for installation. For linux, you'll need to make it an executable by running chmod +x Miniconda3-latest-Linux-x86_64.sh and then running the installer script with ./Miniconda3-latest-Linux-x86_64.sh in your terminal.","Create a conda environment for TextGen:","If you are, replace the third command with this: pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.2","Run the web server:","By default, oobabooga's TextGen will load the first model (in alphabetical order) placed in your models folder. You can specify which model to load with the --model argument, followed by your model name as it's named in the models folder. Here's an example of how you'd run Pygmalion-6b:","python server.py --model PygmalionAI/pygmalion-6b --cai-chat --auto-devices --no-stream","The PygmalionAI/ part is for if you want to download the model first. If you already have pygmalion-6b downloaded in your models folder, then simply do --model pygmalion-6b.","Please don't forget to pass the --load-in-8bit argument too if you have a low VRAM PC! --auto-devices should take care of the memory assignment if you have less 10GB VRAM.","You can view the full list of commands here"]}],[{"l":"TavernAI","p":["TavernAI is an atmospheric front-end that connects to KoboldAI, NovelAI, or OpenAI (but doesn't run the models itself, as opposed to KoboldAI and Oobabooga's TextGen)."]},{"l":"Installation","p":["Installing TavernAI is simple. It officially supports the following operating systems:","Windows x64","Linux x64","MacOS (Darwin x64)"]},{"l":"Windows Installation","p":["Download TavernAI and run the installer:","TavernAI","You'll need a program such as 7zip to extract the installer."]},{"i":"linuxmacos-installation","l":"Linux/MacOS Installation","p":["If you're on Linux, an automatic installer script is now available in the main Tavern repo. It's still recommended to use your package manager, as the script might mess up if your environment is very custom-made (or if you're using NixOS). If you already have nodejs installed, or would like to use the script, please skip the nodejs installation section."]},{"l":"Requirements","p":["nodejs","git"]},{"i":"installation-1","l":"Installation","p":["Download nodejs via your package manager of choice.","Clone the repo","Run TavernAI","Connect your KoboldAI instance (works both locally and with Colab)","If you're running KoboldAI locally, all you need to paste in there is http://localhost:5000/api. If you're using Google Colab, copy your remote URL instead. If it ends with a # or new_ui, remove them and replace them with /api. If they don't, simply adding /api will suffice.","You can now start using TavernAI! TavernAI uses character cards, which are images (PNG and WEBP) that can be imported to Tavern as Characters. Tavern 1.3 by default has a list of community-added Characters in the home page (provided you have an internet connection). You can also find new characters in the unofficial Discord server."]}],[{"l":"Output Length","p":["Output Length is a setting used to control the length of the output generated by the model. The output length setting will determine how long the model's reply will be based on the value you set.","For example, if you set the output length to 200, the model will generate a piece of text that is approximately 100 tokens long, depending on the specifics of the model and the input data.","The Output Length setting can be useful for controlling the amount of output generated by the model and for ensuring that the generated text is of a consistent length. A higher value will take longer to generate."]}],[{"l":"Temperature","p":["Temperature determines the randomness of sampling. A higher value can increase creativity, but can make the output less meaningful and coherent. Lower values will make the output more predictable, but it may become more repetitive."]}],[{"l":"Top P Sampling","p":["Top_P is a way of generating text with an AI model that helps to make the output more diverse and interesting. When using Top_P, you select the most likely words to come next based on their probability of occurring, but only up to a certain cumulative probability threshold. This means that instead of always selecting the most likely word, the language model can choose from a wider range of words that are still reasonably likely to occur.","For example, if you set Top_P to 0.5, the language model will only consider words that make up the top 50% of the probability distribution for the next word, rather than just the most likely word.","You can can use Top_P to make the model output more varied and creative. By not always selecting the most likely word, the language model can surprise you with unexpected but still plausible words, creating more interesting and engaging text."]}],[{"l":"Top K Sampling","p":["When using \"top k\", you select the top k most likely words to come next based on their probability of occurring, where k is a fixed number that you specify.","For example, if you set Top_K to 5, the language model will only consider the five most likely words to come next, rather than all the words in its vocabulary.","You can use Top_K to control the amount of diversity in the model output. By limiting the selection to a small number of the most likely words, the language model can produce more predictable and conservative text. However, by increasing k, you can allow for more unexpected and creative choices in the generated text.","Overall, Top_K can be useful for balancing the tradeoff between predictability and creativity in language model output."]},{"l":"Combining Top_K with Top_P","p":["Combining Top_K and Top_P is a technique called nucleus sampling or Top_P sampling with Top_K cutoff. This technique allows you to use both methods together to generate even more diverse and interesting text.","Here's how it works: first, the model uses Top_p to select the most likely words up to a certain cumulative probability threshold, as explained in the Top_P page","Then, it applies a Top_K cutoff to this set of words, selecting only the top k most likely words from the Top_P selection.","By using both techniques together, you can generate text that is both diverse and controlled. The Top_P selection ensures that the language model has the freedom to choose from a range of plausible words, while the Top_K cutoff limits the number of choices to a manageable set of highly likely words.","The specific values of Top_P and Top_K can be adjusted to achieve different levels of diversity and control in the generated text. For example, a higher Top_P value will allow for more diverse choices, while a lower Top_K value will provide more control over the output. It's important to experiment with different settings to find the right balance for your specific use case."]}],[{"l":"Tail Free Sampling","p":["Tail Free Sampling is a setting used to improve the consistency of the generated output. When generating new text, the model can use Tail Free Sampling to select the next word, but with less emphasis on the rare or extreme values.","The slider is a tool that allows you to adjust the amount of Tail Free Sampling used by the model. When you pick a higher value, the model will be more likely to avoid selecting the rare or extreme values, which can help to increase the consistency of the generated output. This is because the model will be working from the bottom of the probability distribution, selecting more common and likely values and trimming the lowest probability tokens."]}],[{"l":"Typical Sampling","p":["Typical Sampling is a setting used to control how the model selects the next token or word in the generated output. With typical sampling, the model selects the next token based on its probability distribution, meaning that more probable tokens are more likely to be selected.","For example, if the model is generating a sentence and the next probable word is \"the\", the model is more likely to select \"the\" as the next word with typical sampling. This can help to ensure that the generated output is fluent and coherent, as the model is more likely to select words that make sense in the given context."]}],[{"l":"Top A Sampling","p":["Top A Sampling is a way to pick the next word in a sentence based on how important it is in the context. We use a \"leader\" number between 0 and 1 to help us decide which words are most important. Top-A considers the probability of the most likely token, and sets a limit based on its percentage. After this, remaining tokens are compared to this limit. If their probability is too low, they are removed from the pool. Increasing A results in a stricter limit. Lowering A results in a looser limit.","This means that if the top token has a moderate likelihood of appearing, the pool of possibilities will be large. On the other hand, if the top token has a very high likelihood of appearing, then the pool will be 1-3 tokens at most. This ensures that structure remains solid, and focuses creative output in areas where it is actually wanted."]}],[{"l":"Repetition Penality","p":["Repetition Penalty is a setting used to control how often the model repeats certain words or phrases in the generated output. When the repetition penalty is set to a higher value, the model is less likely to repeat the same word or phrase multiple times in the output."]},{"l":"Repetition Penalty Range","p":["If set higher than 0, repetition penality only applies to the last few tokens of the story rather than to the entire story. The slider controls the amount of tokens at the end of your story to apply it to."]},{"l":"Repetition Penalty Slope","p":["If both this setting and Repetition Penality Range are set higher than 0, the repetition penality will be apply more strongly on tokens that are close to the end of the story. High values will result in the repetition penalty difference between the start and the end of your story being more apparent."]}],[{"l":"Alt Rep Pen","p":["Alt Rep Pen applies Repetition Penalty as a logarithmic modifier rather than a linear modifier. This means that the penalty will be smaller for the first few repetitions and will increase more slowly as the number of repetitions increases.","In simpler terms, the more your model repeats a token, the more severe the penalty becomes."]}],[{"l":"Context Tokens","p":["Number of tokens to submit to the model for sampling/that are used to generate the output. Make sure this is higher than \"Output Length\". Higher values increase the memory but it also increases the usage of VRAM/RAM."]}],[{"l":"Gens Per Action","p":["The number of outputs the model will generate at once. Use this setting if your responses are too short; it will make the bot generate twice in order. Increases VRAM/RAM usage."]}],[{"i":"wi-world-info-depth","l":"WI (World Info) Depth","p":["Number of actions (Input/Output) from the end of the story to scan for World Info keys.","Excerpt from the KoboldAI wiki:","World Info is where you can flesh out the details of your wider world. The engine will help conserve tokens in your context by only inserting World Info entries when their keywords are mentioned in the actual story text. However, they are inserted toward the top, after the Memory but before the actual story text, so they have a moderate effect on what the AI generates.","World Info entries are like an encyclopedia entry, providing a succinct overview of the most relevant information about whatever topic - characters, species, places, items, etc...","The AI doesn't see the key word or title of the World Info entry \"behind the scenes\", so the actual text should be an entirely self-contained description. For example, if you have a World Info entry titled \"Director Abrams\", the entry should say \"Director Abrams is the executive and governor of the colony\" rather than just \"the executive and governor of the colony\", because the AI will only see the entry.","Use World Info entries that cross reference each other appropriately to create a more dynamic and interactive world. If your \"combat android\" World Info entry mentions that they carry plasma rifles, then create a short \"plasma rifle\" entry separately. If you create a \"Admin Tower\" entry that mentions containing a command center, computer core, and offices, create distinct \"command center\", \"computer core\", and \"offices\" entries that mention being located in the Admin Tower.","It helps to use many key words that might pull in a World Info entry whenever it might be appropriate, rather than just its proper name(s).","If you have a rich tapestry of interconnected World Info entries with many relevant keywords, you may find that multiple World Info entries are getting pulled into the context at any given time. So it's good to keep them fairly short (50 tokens for minor stuff, 100 tokens for significant characters, no more than 150 for major keystones of the setting)."]}],[{"l":"Prefilled Memory","p":["If you make a \"Random Story\" by clicking on the \"New Game\" button with this setting enabled, the memory of the current story will be loaded into the memory of the new Random Story."]}],[{"l":"Chat Mode","p":["This mode optimizes KoboldAI for chatting/using conversational models."]}],[{"l":"Adventure Mode","p":["Turn this on if you are playing a \"Choose Your Adventure\" model. Does not apply to Pygmalion."]}],[{"l":"Dynamic WI Scan","p":["Scans the AI's output for World Info keys as it is generating the output."]}],[{"l":"Token Probabilities","p":["Add a context menu beside the output showing what other words were considered as it was generated."]}],[{"l":"Token Streaming","p":["Shows tokens as they are generated. Creates a similar effect to when other models like ChatGPT or Character.AI display their output in real-time."]}]]