---
order: 1
icon: gear
title: Settings and Parameters
---

You'll find a list of all settings in [KoboldAI](https://docs.alpindale.dev/pygmalion-docs/local-installation-(gpu)/kobold/), Oobabooga's [Text Generation WebUI](https://docs.alpindale.dev/pygmalion-docs/local-installation-(gpu)/oobabooga/), and [SillyTavern](https://docs.alpindale.dev/pygmalion-extras/sillytavern). Please use the Table of Contents to navigate through this page.

!!!info Work in Progress
Currently you'll only find a list of settings in KoboldAI. The other UIs will be added soon but they're more or less the same.
!!!

## KoboldAI

### Output Length

Output Length is a setting used to control the length of the output generated by the model. The output length setting will determine how long the model's reply will be based on the value you set.

For example, if you set the output length to 200, the model will generate a piece of text that is approximately 100 tokens long, depending on the specifics of the model and the input data.

The Output Length setting can be useful for controlling the amount of output generated by the model and for ensuring that the generated text is of a consistent length. A higher value will take longer to generate.
***
### Temperature

Temperature determines the randomness of sampling. A higher value can increase creativity, but can make the output less meaningful and coherent. Lower values will make the output more predictable, but it may become more repetitive.
***
### Top P Sampling

Top_P is a way of generating text with an AI model that helps to make the output more diverse and interesting. When using Top_P, you select the most likely words to come next based on their probability of occurring, but only up to a certain cumulative probability threshold. This means that instead of always selecting the most likely word, the language model can choose from a wider range of words that are still reasonably likely to occur.

For example, if you set Top_P to 0.5, the language model will only consider words that make up the top 50% of the probability distribution for the next word, rather than just the most likely word.

You can can use Top_P to make the model output more varied and creative. By not always selecting the most likely word, the language model can surprise you with unexpected but still plausible words, creating more interesting and engaging text.
***
### Top K Sampling

When using "top k", you select the top k most likely words to come next based on their probability of occurring, where k is a fixed number that you specify.

For example, if you set Top_K to 5, the language model will only consider the five most likely words to come next, rather than all the words in its vocabulary.

You can use Top_K to control the amount of diversity in the model output. By limiting the selection to a small number of the most likely words, the language model can produce more predictable and conservative text. However, by increasing k, you can allow for more unexpected and creative choices in the generated text.

Overall, Top_K can be useful for balancing the tradeoff between predictability and creativity in language model output.

#### Combining Top K with Top P

Combining Top_K and Top_P is a technique called nucleus sampling or Top_P sampling with Top_K cutoff. This technique allows you to use both methods together to generate even more diverse and interesting text.

Here's how it works: first, the model uses Top_p to select the most likely words up to a certain cumulative probability threshold, as explained in the Top_P page

Then, it applies a Top_K cutoff to this set of words, selecting only the top k most likely words from the Top_P selection.

By using both techniques together, you can generate text that is both diverse and controlled. The Top_P selection ensures that the language model has the freedom to choose from a range of plausible words, while the Top_K cutoff limits the number of choices to a manageable set of highly likely words.

The specific values of Top_P and Top_K can be adjusted to achieve different levels of diversity and control in the generated text. For example, a higher Top_P value will allow for more diverse choices, while a lower Top_K value will provide more control over the output. It's important to experiment with different settings to find the right balance for your specific use case.
***

### Tail Free Sampling
Tail Free Sampling is a setting used to improve the consistency of the generated output. When generating new text, the model can use Tail Free Sampling to select the next word, but with less emphasis on the rare or extreme values.

The slider is a tool that allows you to adjust the amount of Tail Free Sampling used by the model. When you pick a higher value, the model will be more likely to avoid selecting the rare or extreme values, which can help to increase the consistency of the generated output. This is because the model will be working from the bottom of the probability distribution, selecting more common and likely values and trimming the lowest probability tokens.
***
### Typical Sampling
Typical Sampling is a setting used to control how the model selects the next token or word in the generated output. With typical sampling, the model selects the next token based on its probability distribution, meaning that more probable tokens are more likely to be selected.

For example, if the model is generating a sentence and the next probable word is "the", the model is more likely to select "the" as the next word with typical sampling. This can help to ensure that the generated output is fluent and coherent, as the model is more likely to select words that make sense in the given context.
***
### Top A Sampling
Top A Sampling is a way to pick the next word in a sentence based on how important it is in the context. We use a "leader" number between 0 and 1 to help us decide which words are most important. Top-A considers the probability of the most likely token, and sets a limit based on its percentage. After this, remaining tokens are compared to this limit. If their probability is too low, they are removed from the pool. Increasing A results in a stricter limit. Lowering A results in a looser limit.

This means that if the top token has a moderate likelihood of appearing, the pool of possibilities will be large. On the other hand, if the top token has a very high likelihood of appearing, then the pool will be 1-3 tokens at most. This ensures that structure remains solid, and focuses creative output in areas where it is actually wanted.
***
### Repetition Penalty
Repetition Penalty is a setting used to control how often the model repeats certain words or phrases in the generated output. When the repetition penalty is set to a higher value, the model is less likely to repeat the same word or phrase multiple times in the output.

#### Repetition Penalty Range

If set higher than 0, repetition penality only applies to the last few tokens of the story rather than to the entire story. The slider controls the amount of tokens at the end of your story to apply it to.

#### Repetition Penalty Slope

If both this setting and Repetition Penality Range are set higher than 0, the repetition penality will be apply more strongly on tokens that are close to the end of the story. High values will result in the repetition penalty difference between the start and the end of your story being more apparent.
***
### Alt Rep Pen
Alt Rep Pen applies Repetition Penalty as a logarithmic modifier rather than a linear modifier. This means that the penalty will be smaller for the first few repetitions and will increase more slowly as the number of repetitions increases.

In simpler terms, the more your model repeats a token, the more severe the penalty becomes.
***
### Context Tokens
Number of tokens to submit to the model for sampling/that are used to generate the output. Make sure this is higher than "Output Length". Higher values increase the memory but it also increases the usage of VRAM/RAM.
***
### Gens Per Action
The number of outputs the model will generate at once. Use this setting if your responses are too short; it will make the bot generate twice in order. Increases VRAM/RAM usage.
***
### WI Depth (World Info)
Number of actions (Input/Output) from the end of the story to scan for World Info keys.

Excerpt from the [KoboldAI wiki](https://github.com/KoboldAI/KoboldAI-Client/wiki/Memory,-Author's-Note-and-World-Info#world-info):

World Info is where you can flesh out the details of your wider world. The engine will help conserve tokens in your context by only inserting World Info entries when their keywords are mentioned in the actual story text. However, they are inserted toward the top, after the Memory but before the actual story text, so they have a moderate effect on what the AI generates.

World Info entries are like an encyclopedia entry, providing a succinct overview of the most relevant information about whatever topic - characters, species, places, items, etc...

The AI doesn't see the key word or title of the World Info entry "behind the scenes", so the actual text should be an entirely self-contained description. For example, if you have a World Info entry titled "Director Abrams", the entry should say "Director Abrams is the executive and governor of the colony" rather than just "the executive and governor of the colony", because the AI will only see the entry.

Use World Info entries that cross reference each other appropriately to create a more dynamic and interactive world. If your "combat android" World Info entry mentions that they carry plasma rifles, then create a short "plasma rifle" entry separately. If you create a "Admin Tower" entry that mentions containing a command center, computer core, and offices, create distinct "command center", "computer core", and "offices" entries that mention being located in the Admin Tower.

It helps to use many key words that might pull in a World Info entry whenever it might be appropriate, rather than just its proper name(s).

If you have a rich tapestry of interconnected World Info entries with many relevant keywords, you may find that multiple World Info entries are getting pulled into the context at any given time. So it's good to keep them fairly short (50 tokens for minor stuff, 100 tokens for significant characters, no more than 150 for major keystones of the setting).
***
### Prefilled Memory
If you create a "Random Story" by clicking on the "New Game" button with this setting enabled, the memory of the current story will be loaded into the memory of the new Random Story.
***
### Chat Mode
This mode optimizes KoboldAI for chatting/using conversational models. Use this mode for Pygmalion or other chatbot models.
***
### Adventure Mode
Turn this on if you are playing a "Choose Your Adventure" model. Does not apply to Pygmalion.
***
### Dynamic WI Scan (World Info)
Scans the AI's output for World Info keys as it is generating the output. If you disable this, KoboldAI will only scan the output after the bot has finished generating the text.
***
### Token Probabilities
Add a context menu beside the output showing what other words were considered as it was generated.
***
### Token Streaming

Shows tokens as they are generated. Creates a similar effect to when other models like ChatGPT or Character.AI display their output in real-time.