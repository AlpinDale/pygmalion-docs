[[{"i":"what-is-pygmalionai","l":"What is PygmalionAI?","p":["PygmalionAI is an open-source L arge L anguage M odel (LLM) based on EleutherAI's GPT-J 6B.","In simple terms, Pygmalion is an AI fine-tuned for chatting and Role-Playing purposes. The current actively supported Pygmalion AI model is the 6B (6 Billion Parameters) variant."]},{"i":"how-do-i-use-pygmalion","l":"How do I use Pygmalion?","p":["Language models, including Pygmalion, generally run on Graphics Processing Units (GPUs), since they need access to fast memory and processing power to output coherent text at an acceptable speed. Pygmalion is no different. You need a powerful GPU to run the model.","If you don't have a powerful enough GPU, there are luckily alternative solutions. Google's Colaboratory offers free GPUs for all users - though for a limited time. We have ready-made notebooks that you can run to access Pygmalion. You can also use Colab on your mobile phone.","These docs will attempt to explain all the possible methods for running the AI, and in a language that is easy to understand. We will start with Google Colab, as it's generally the easiest method."]}],[{"l":"Colab Notebooks","p":["Running Pygmalion - or any Large Language Model - requires a significant amount of VRAM. The current absolute minimum requirement is 10GB for a comfortable experience. Anything lower, and you would either be unable to use the model, or generations will be too slow to be enjoyable.","Google has generously offered free GPUs for in their Colaboratory. The Colab's free plan offers an NVIDIA T4 (16GB) GPU, which is more than enough to run the 6B Pygmalion Model. A TPU v2-8 is also available in the free plan, but availibility is low due to high demand."]},{"l":"Notebooks","p":["There are currently two backends to run the Pygmalion 6B model - KoboldAI and Text-Generation-WebUI by oobabooga.","TavernAI is a frontend that connects to KoboldAI and essentially facilitates a connection to Pygmalion via Kobold while offering a pleasant looking User Interface. TavernAI by itself doesn't run Pygmalion - therefore you do not need a powerful PC to run TavernAI. You cannot use TavernAI with oobabooga's Text-Gen-WebUI.","If you're on Mobile, use either oobabooga's TextGen or TavernAI.","Click here for the notebook link, and here for the guide.","\uD83D\uDCF1 Click here.","\uD83D\uDCBB Click here.","When you're done using Pygmalion, please terminate your Colab session! You'll waste your quota otherwise, and might find yourself unable to connect to a GPU backend the next time you login."]}],[{"l":"KoboldAI","p":["Each Colab notebook will greet you with a guide. For the sake of simplicity, visual guides will be included below. You can find the notebook here. Middle click or right-click and open in a new tab if you want to follow along with this guide!","Make sure you're signed in to your Google Account before you attempt running any of the notebooks.","Scroll down and select the highlighted options, and then run the cell:","You might be warned that the notebook is not authored by Google - you can ignore it and click on Run anyway.","You'll be prompted to give the notebook access to your Google Drive files. Click on Connect to Google Drive. This will open a pop-up browser window where you'll give Colab access to your Google Account. Select the account you wish to use, then scroll down and click on \"Allow\".","Wait for the notebook to finish running. This can take up to 5 or 10 minutes.","Once it's loaded, two URLs will appear. If you wish to use KoboldAI with Tavern, please copy the first URL. If you wish to use Kobold by itself, please click on the second URL.","You can start using Pygmalion if you clicked on the second link. If you want to use it with Tavern, please refer to the TavernAI guide included here."]}],[{"l":"Text Generation WebUI","p":["activate_character_bias: Lets you add a snippet of text that'll be put before every reply your bot generates - but keeps it hidden from you. In simple terms, if you add *yells* in the character bias section, your character will yell every line.","activate_sending_pictures: Allows you to send pictures to your bots.","activate_silero_text_to_speech: Use this option if you want TTS for your bots.","cai_chat: Will mimic the appearance of Character.AI's user interface.","chat_language: Pygmalion is a primarily English-only model - this feature simply runs yours and your bot's responses through a translation service.","Each Colab notebook will greet you with a guide. For the sake of simplicity, visual guides will be included below. You can find the notebook here. Middle click or right-click and open in a new tab if you want to follow along with this guide!","If you're on mobile, run this cell and keep the audio file playing:","load_in_8bit: Pygmalion 6B's weights are stored in Floating Point 16 bits, this will load them in Integer 8 bits instead - which cuts down on the computation power required. Use this so you can increase context size to the maximum.","Make sure you're signed in to your Google Account before you attempt running any of the notebooks.","model: Use this option to select which version of the Pygmalion model you want - original is the first version of the model, main is the current stable release (v3), dev is the latest beta release.","Once the model is loaded, you can find your URL here:","Run the second cell, and make sure the save_logs_to_google_drive option is checked. If you don't check this, your characters and chatlogs won't persist across sessions on the same account:","Scroll down to the third cell and finally run Pygmalion:","text_streaming: The generated text by the bot is displayed in real-time instead of waiting for the whole message to finish generating before showing it to you.","Wait for the cell to finish loading Pygmalion. This cell will not stop running until you terminate your Colab session or manually stop it.","Wait for the cell to finish running. You can find out whether it's finished by checking if the cell button is still spinning:","Wait for the checkpoint shards to be loaded. This should take a few minutes at most.","You can now start chatting with any bot you want. You can find bots in our unofficial Discord Server.","You might be warned that the notebook is not authored by Google - you can ignore it and click on Run anyway.","You'll be prompted to give the notebook access to your Google Drive files. Click on Connect to Google Drive. This will open a pop-up browser window where you'll give Colab access to your Google Account. Select the account you wish to use, then scroll down and click on \"Allow\"."]}],[{"l":"TavernAI","p":["Each Colab notebook will greet you with a guide. For the sake of simplicity, visual guides will be included below. You can find the notebook here. Middle click or right-click and open in a new tab if you want to follow along with this guide!","Make sure you're signed in to your Google Account before you attempt running any of the notebooks.","If you're on mobile, run this cell and keep the audio file playing:","You might be warned that the notebook is not authored by Google - you can ignore it and click on Run anyway.","Scroll down and run this cell to launch Pygmalion with Tavern:","Choose Pygmalion Dev in the Model drop-down menu if you want to try the latest beta release of the model.","You'll be prompted to give the notebook access to your Google Drive files. Click on Connect to Google Drive. This will open a pop-up browser window where you'll give Colab access to your Google Account. Select the account you wish to use, then scroll down and click on \"Allow\".","Wait around 5 minutes for TavernAI to finish loading. You can find the URL here at the end:","You can now start chatting with any character you wish! TavernAI offers community-created characters, but you can also add your own by clicking on the burger menu to the top-right corner of the screen, and then the +Import button in the Characters menu. Find more characters in the unofficial Discord server."]}],[{"l":"Overview","p":["16GB","20GB","24GB","AMD","Consumer-grade (Gaming) GPUs:","GPU","If you don't have any of these cards, but still have over 8GB of VRAM, chances are you can still run the model just fine. You just have to make sure your card supports CUDA (NVIDIA) or ROCm (AMD). We'll get to that later.","Manufacturer","NVIDIA","Please continue to the GPU set-up section. Setting up your GPU","PygmalionAI is a Large Language Model, and as the name might imply, it needs a lot of computation power to run it. You need a minimum of 16GB VRAM to run the model -- without any optimizations, that is. Here's a list of all the GPUs that would work without any tweaks and out-of-the-box:","Radeon RX 6800","Radeon RX 6800 XT","Radeon RX 6900 XT","Radeon RX 6950 XT","Radeon RX 7900 XT","Radeon RX 7900 XTX","RTX 3090","RTX 3090 Ti","RTX 4080","RTX 4090","Titan RTX","VRAM"]}],[{"l":"Setting up your GPU","p":["Consumer-grade GPUs are designed for Video Processing and 3D rendering - they're not specifically made for Language Processing models. However, GPU vendors such as NVIDIA and AMD have released toolkits for enabling computation for their GPUs. Only a handful of AMD GPUs are supposed, but the majority of NVIDIA cards can be setup easily.","Keep in mind that the most important factor for AI is VRAM (Video Memory), not the processing power. e.g. an RTX 2070 (8GB) is better than an RTX 4050 (6GB) when it comes to AI inference.","Please refer to the links below to check if your GPU is compatible:","NVIDIA","AMD","If your GPU is supported, please proceed with the installation."]},{"l":"NVIDIA","p":["NVIDIA Cards are mostly supported - though it's not recommended to use GPUs with less than 8GB of VRAM.","The setup process will be different for both Windows and Linux (MacOS currently not supported)."]},{"l":"Windows","p":["The official Game-Ready Drivers provided by NVIDIA will setup CUDA for you, and that's all you need to do!"]},{"l":"Linux","p":["CUDA installation is relatively easy for Linux (unless you're on Fedora). For this particular guide, we'll be covering three base distros - Arch Linux and Debian. More will be added later."]},{"l":"Arch Linux","p":["Install the NVIDIA drivers (if they aren't already installed):","If you're on the LTS Arch Linux Kernel, then please install nvidia-lts instead. You can find out your Kernel by running uname -r","Install CUDA and (optionally) cuDNN:","You're done! Verify installation by running nvcc -v."]},{"l":"Debian","p":["Install the NVIDIA drivers (if they aren't already installed):","Install CUDA:","You're done! Verify installation by running nvcc -v."]},{"l":"AMD","p":["ROCm is (currently) not supported by Windows. You cannot run any language model on Windows if you have an AMD card - this isn't a Pygmalion-only issue. You will need to either dual-boot Linux or switch to Linux entirely if you wish to run language models on your AMD GPU."]},{"i":"linux-1","l":"Linux","p":["The ROCm installation is only (relatively) easy on Arch Linux. For other distros, you need to manually install the binaries by building ROCm from source (not recommended for beginners).","Installing Arch Linux is not easy for a beginner, therefore look into Manjaro or EndeavourOS- they both use an Arch Linux base."]},{"i":"arch-linux-1","l":"Arch Linux","p":["Install GPU drivers by following the official guide.","Install an AUR Helper. We'll use paru for this guide:","Install ROCm:"]},{"i":"i-dont-have-arch-linux","l":"I don't have Arch Linux","p":["AMD has a comprehensive guide for installing ROCm. Please refere to their guide on the installation."]}],[{"l":"Frequently Asked Questions"},{"i":"my-character-has-terrible-memory","l":"My character has terrible memory!","p":["This could be due to the limited context size. Pygmalion 6B has a maximum context size of 2048 tokens. This includes your character description, example messages, and all your chatlogs. The description and examples are placed at the top of the context memory - all your subsequent chat logs are placed beneath them. This means that if your character description + examples chats are 400 tokens, you'll only have 1648 tokens left for your messages. The bot will forgot everything past those 1648 tokens.","Character editors/creators such as TavernAI's will give you a token count for your character, but you can also use OpenAI's tokenizer service.","OpenAI Tokenizer"]},{"i":"my-characters-responses-are-too-short","l":"My character's responses are too short!","p":["The character will mimic the example chats and greeting message's style. Keep these in mind if you want to create a verbose character:","Descriptive and verbose greeting message and example chats.","Descriptive and verbose responses by the user (you!).","Re-generate the response until you see a satisfactory one.","Manually edit the character's response to make it more verbose."]},{"i":"my-character-is-generating-nonsense","l":"My character is generating nonsense!","p":["Your Temperature or Repetition Penalty settings are likely too high. The recommended range for Temperature (for chatbots) is between 0.5 to 0.9. The range for Repetition Penalty is between 1.1 to 1.2."]},{"i":"my-character-is-not-responding-anymore","l":"My character is not responding anymore!","p":["You have probably run out of memory (VRAM). If you are using the free Google Colab plan, the GPU provided by Google can't handle the maximum context size (2048 tokens). If you have 16GB of VRAM, please tweak the context size to 1400-1600 (proportionally lower according to your VRAM size)."]},{"i":"for-text-gen-oobabooga-you-need-to-check-the-load_in_8bit-option-under-the-3rd-cell-before-running-it","l":"For Text-Gen (Oobabooga) you need to check the load_in_8bit option under the 3rd cell before running it."},{"i":"for-tavernai-running-the-model-in-8bit-is-not-available-the-only-solution-is-to-slide-down-the-context-size-to-around-1400-or-1600-in-the-settings-tab","l":"For TavernAI, running the model in 8bit is not available. The only solution is to slide down the context size to around 1400 (or 1600) in the settings tab."},{"i":"do-you-keepshare-any-of-my-datachat","l":"Do you keep/share any of my data/chat?","p":["The Pygmalion project team does not collect any data other than the chat logs you explicitly consent to donating for the training dataset. This project is done out of passion, and no one has any intention of collecting, analyzing, or selling any of your data. The Colab notebook automatically passes the --quiet argument in KoboldAI, which means your/the bot's responses aren't printed out for Google to see."]},{"i":"what-are-softprompts-do-i-need-them","l":"What are Softprompts? Do I need them?","p":["A soft prompt is a way to modify the style and behavior of your AI.","They're made by taking a (possibly) huge amount of information, and compressing them into a small number of tokens. Even if people tend to use it that way, softprompt are not made to add context, lore, etc to the story. But yes, it might work for that if the dataset use to make the softprompt was big enough and made use of good quality data."]},{"i":"how-do-i-use-softprompts","l":"How do I use Softprompts?","p":["Download your SP of choice from either the Discord or this Rentry a generous anon is maintaining.","If you're running locally: place the .zip file (don't extract!) inside the softprompts folder - both Oobabooga and KoboldAI have the same folder.","If you're using Google Colab, open your Google Drive and find the KoboldAI/softprompts folder and upload your .zip file there.","Make sure Pygmalion is loaded, then an option for Softprompts should appear."]},{"i":"which-ui-should-i-use-theres-so-many-of-them","l":"Which UI should I use? There's so many of them.","p":["It's up to you, every UI tend to add the same features as the other and try to be compatible with character from other UI as well. If you want to focus on Story Generation, we recommend using KoboldAI. For chat purposes, TavernAI has the best user interface, followed by Oobabooga. There are other TavernAI alternatives (e.g. miku.gg), but it's up to you in the end. Please refer to their Colab Pages for a preview of the UI."]},{"i":"im-using-google-colab-how-much-quota-do-i-have","l":"I'm using Google Colab, how much quota do I have?","p":["The amount of time you can use when using the free plan for Google Colab highly depends on the traffic and your usage patterns. Google says that you can use a Colab Notebook for at most 12 hours if you don't have Compute Unit. Pygmalion users tend to say it's less."]},{"i":"how-to-avoid-running-out-of-quota","l":"How to avoid running out of quota?","p":["When you're done using Pygmalion, please terminate your Colab session! You'll waste your quota otherwise, and might find yourself unable to connect to a GPU backend the next time you login."]},{"i":"ive-out-of-quota-what-do-i-do-now","l":"I've out of Quota, what do I do now?","p":["If you've run out of quota, you can do nothing but wait for your quota to reset. Alternatively, you can purchase Colab Pro to increase your daily/weekly quota."]},{"i":"whats-a-compute-unit-cu-should-i-buy-one","l":"What's a Compute Unit (CU)? Should I buy one?","p":["A CU is a unit that you can buy to be sure to be granted access to a powerful GPU. Pricing for Compute Engine is based on per-second usage of the machine types, persistent disks, and other resources that you select for your virtual machines. So it's up to you."]},{"i":"ive-run-out-of-quota-but-i-cant-buy-a-compute-unit-what-do-i-do","l":"I've run out of quota, but I can't buy a Compute Unit! What do I do?","p":["In that case, you can use Kobold Horde. Generous users are donating their GPU power to the Horde, so that people can use them to run Language Models. Pygmalion is a popular model, so you'll always find people hosting it. Keep in mind that there are more Horde users than Workers, so you might have to wait a bit longer for responses."]}]]