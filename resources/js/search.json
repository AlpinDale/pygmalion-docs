[[{"i":"what-is-pygmalionai","l":"What is PygmalionAI?","p":["PygmalionAI is an open-source large language model ( LLM) based on EleutherAI's GPT-J 6B and Meta AI's LLaMA models.","In simple terms, Pygmalion is an AI fine-tuned for chatting and roleplaying purposes. The current actively supported Pygmalion AI model is the 7B variant, based on Meta AI's LLaMA model."]},{"l":"Features","p":["Unrestricted: No measures have been put in place to restrict the output.","Low VRAM requirement: With only 18GB (or less) VRAM required, Pygmalion offers better chat capability than much larger language models with relatively minimal resources.","Fine-tuned for RP: Our curated dataset of high-quality roleplaying data ensures that your bot will be the optimal RP partner.","Free and Open-Source: Both the model weights and the code used to train it are completely open-source, and you can modify/re-distribute it for whatever purpose you want.*","Regularly updated: Pygmalion is regularly updated with new data to improve the model even further.","* Pygmalion-6B uses the CreativeML Open RAIL-M license, which is also used by Stable Diffusion. Pygmalion-7B uses LLaMA's license, which is non-commercial."]},{"i":"how-do-i-use-pygmalion","l":"How do I use Pygmalion?","p":["Language models, including Pygmalion, generally run on GPUs since they need access to fast memory and massive processing power in order to output coherent text at an acceptable speed. Pygmalion is no different. You need a powerful GPU to run the model.","If you don't have a powerful enough GPU, there are alternative solutions. Running the models at lower precision allows even low-vram GPUs to be utilized, and with frameworks such as GGML, running the Pygmalion models purely on CPU has been made possible.","These docs will attempt to explain all the possible methods for running the AI, and in a language that is easy to understand. Please navigate to the next section to begin using Pygmalion."]}],[{"l":"Quickstart","p":["There are multiple ways to run a large language model like Pygmalion, including using cloud computing instances or locally installed software. For users who want to run the model locally, it's important to ensure that their computer meets the minimum requirements, including having a GPU with enough memory. Outlined below are instructions for Windows Users to find out which criteria they meet so they can quickly get started."]},{"l":"Mobile","p":["You have a few options for running Pygmalion on your mobile phone, depending on what phone you're using."]},{"i":"ios-iphone","l":"iOS (iPhone)","p":["Your only choice is to use Kobold Horde. You can use the Agnaistic website to get started:","Kobold Horde","Keep in mind that the Kobold Horde is run by generous donors running Pygmalion on their own PCs and allowing other people to use them. The number of users far exceeds the number of hosts. You're encouraged to also chip in by hosting Pygmalion yourself if you meet the required specifications to run it locally on your PC."]},{"l":"Android","p":["Android users have more options than iOS users. You can either run the model locally on your phone, no internet connection required, or use the Kobold Horde via SillyTavern/Agnaistic."]},{"l":"Locally","p":["To run Pygmalion locally on your phone, you'll need to make sure you have at least 8GB of RAM. You can find out the memory size for your phone by looking it up online. You can refer to our guide here for instructions on how to set it up:","koboldcpp"]},{"l":"SillyTavern","p":["You can also run Pygmalion using SillyTavern via Kobold Horde. Simply follow the Android instructions in SillyTavern guide:"]},{"i":"local-pc","l":"Local (PC)"},{"l":"Identifying your GPU","p":["Find out what GPU you're using (if any at all) by performing the following steps:"]},{"l":"Windows","p":["Press the Windows Key+ R. This will open the Run window.","Type in dxdiag and press enter.","You might be greeted with a dialogue box if it's the first time launcing dxdiag. You can pick whatever option you want, it doesn't matter.","Look for the Display tab on the top side of the window. Depending on your PC/Laptop, there might be two Display tabs - aptly named \"Display 1\" and \"Display 2\". In that case, click on \"Display 2\" instead.","Look for the `Display Memory (VRAM) part of the screen.","Proceed to the next section","Unfortunately, the framework required to enable compute on AMD GPUs (ROCm) is currently unavailable on Windows. While we wait for AMD to add ROCm support to Windows, you will either have to switch to Linux or use the Cloud/C++ solutions."]},{"l":"Linux","p":["Use the following command to find out whether you have a discrete GPU or not:","If you have a dedicated GPU, you should see something like this; an NVIDIA card with the Kernel drive in use being nvidia:","In that case, run nvidia-smi to view your VRAM amount. Then continue to the next section. If the kernel driver in use is not nvidia or nvidia-lts, you will need to install the nvidia drivers for your distro.","If you don't have a discrete GPU, you would see something like this: In that case, proceed to this section"]},{"i":"what-to-do-now","l":"What to do now?","p":["There are several options, depending on how much VRAM your system has. Outlined below are the multiple scenarios and the recommended method of running Pygmalion for each. Please use the Table of Contents for navigating."]},{"i":"i-have-no-vram","l":"I have no VRAM!","p":["In this case, your only choice is to either run on the Cloud or use Pygmalion C++ to run the model on your CPU."]},{"l":"Cloud","p":["There are several options for running on the cloud - Google Colab and GPU rental services.","Google Colab is free, but there are restrictions for the end user. Namely: daily/weekly usage quotas and a less powerful GPU. If you want to take this route, please refer to either the TextGen WebUI for running Pygmalion itself or KoboldAI Notebooks for the various Mixed Models.","GPU Rental services, such as vast.ai are another option. They're paid services, but with the added benefits of powerful GPUs, virtually no quotas (since you pay hourly and on-the-go), and next to no restrictions on your usage. The included guide for vast.ai in these docs provide a docker image for KoboldAI which will make running Pygmalion very simple for the average user."]},{"i":"pygmalion-c","l":"Pygmalion C++","p":["If you have a decent CPU, you can run Pygmalion with no GPU required, using the GGML framework for Machine Learning. Please refer to the guide for instructions on how to proceed."]},{"i":"i-have-less-than-4gb","l":"I have less than 4GB!","p":["In that case, please refer to the section above for Cloud and C++ options."]},{"i":"i-have-6gb-or-more-but-less-than-10gb","l":"I have 6GB or more but less than 10GB!","p":["Please refer to the 4-bit guide. Alternatively, you can also use one of the Cloud options if you find the 4bit model undesirable."]},{"i":"i-have-10gb-or-more-but-less-than-16gb","l":"I have 10GB or more but less than 16GB!","p":["Please refer to the TextGen WebUI guide to run Pygmalion at 8bit precision. Make sure you pass the --load-in-8bit argument when launching the WebUI. Alternatively, if you're using Linux, you can also use KoboldAI for 8-bit precision mode. Please refer to the 4-bit guide for instructions."]},{"i":"i-have-16gb-or-more","l":"I have 16GB or more!","p":["You have the recommended amount of VRAM for the 6B model. You may pick whatever option you want - all the guides here will work for you. You can get started here."]}],[{"l":"Frequently Asked Questions"},{"i":"how-do-i-use-the-latest-dev-v8-version","l":"How do I use the latest dev (v8) version?","p":["If you're running on Google Colab, all notebooks have an option for downloading the dev(beta) model. Look for either Pygmalion 6B Dev or Pygmalion 6B Experimental.","If you're running locally, make sure git is installed and run the following command inside your KoboldAI/models or text-generation-webui/models folder:","If it says git lfs was not recognized as a command, please install git-lfs with your package manager (Linux) or download the installer for Windows."]},{"i":"my-character-has-terrible-memory","l":"My character has terrible memory!","p":["This could be due to the limited context size. Pygmalion 6B has a maximum context size of 2048 tokens. This includes your character description, example messages, and all your chatlogs. The description and examples are placed at the top of the context memory - all your subsequent chat logs are placed beneath them. This means that if your character description + examples chats are 400 tokens, you'll only have 1648 tokens left for your messages. The bot will forget everything past those 1648 tokens.","Character editors/creators such as TavernAI's will give you a token count for your character, but you can also use OpenAI's tokenizer service.","OpenAI Tokenizer"]},{"i":"my-characters-responses-are-too-short","l":"My character's responses are too short!","p":["The character will mimic the example chats and greeting message's style. Keep these in mind if you want to create a verbose character:","Be descriptive and verbose in your greeting message and example chats.","Be descriptive and verbose in responses by the user (you!).","Re-generate the response until you see a satisfactory one.","Manually edit the character's response to make it more verbose."]},{"i":"my-character-is-generating-nonsense","l":"My character is generating nonsense!","p":["Your temperature or repetition penalty settings are likely too high. The recommended range for temperature (for chatbots) is between 0.5 to 0.9 and the ideal range for repetition penalty is between 1.1 to 1.2."]},{"i":"my-character-is-not-responding-anymore","l":"My character is not responding anymore!","p":["You have probably run out of GPU memory (also known as VRAM). If you are using the free Google Colab plan, the GPU provided by Google can't handle the maximum context size (2048 tokens). If you have 16GB of VRAM, please tweak the context size to 1400-1600 (proportionally lower according to your VRAM size)."]},{"i":"for-text-gen-oobabooga-you-need-to-check-the-load_in_8bit-option-under-the-3rd-cell-before-running-it","l":"For Text-Gen (Oobabooga) you need to check the load_in_8bit option under the 3rd cell before running it."},{"i":"for-tavernai-running-the-model-in-8bit-is-not-available-the-only-solution-is-to-slide-down-the-context-size-to-around-1400-or-1600-in-the-settings-tab","l":"For TavernAI, running the model in 8bit is not available. The only solution is to slide down the context size to around 1400 (or 1600) in the settings tab."},{"i":"do-you-keepshare-any-of-my-datachat","l":"Do you keep/share any of my data/chat?","p":["The Pygmalion project team does not collect any data other than the chat logs you explicitly consent to donating for the training dataset. This project is done out of passion, and no one has any intention of collecting, analyzing, or selling any of your data. The Colab notebook automatically passes the --quiet argument in KoboldAI, which means your/the bot's responses aren't printed out for Google to see."]},{"i":"what-are-softprompts-do-i-need-them","l":"What are Softprompts? Do I need them?","p":["A soft prompt is a way to modify the style and behavior of your AI.","They're made by taking a (possibly) huge amount of information and compressing them into a small number of tokens, then feeding it into the embedding layer of the model. Even if people tend to use it that way, softprompt are not made to add context, lore, etc to the story. But yes, it might work for that if the dataset use to make the softprompt was big enough and made use of good quality data."]},{"i":"how-do-i-use-softprompts","l":"How do I use Softprompts?","p":["Download your SP of choice from either the Discord or this Rentry a generous anon is maintaining.","If you're running locally: place the .zip file (don't extract!) inside the softprompts folder - both Oobabooga and KoboldAI have the same folder.","If you're using Google Colab, open your Google Drive and find the KoboldAI/softprompts folder and upload your .zip file there.","Make sure Pygmalion is loaded, then an option for Softprompts should appear."]},{"i":"which-ui-should-i-use-theres-so-many-of-them","l":"Which UI should I use? There's so many of them.","p":["It's up to you, every UI tends to add the same features others as well as try to be compatible with characters from other UIs as well. If you want to focus on story generation, we recommend using KoboldAI. For chat purposes, TavernAI has the best user interface, followed by Oobabooga. There are other TavernAI alternatives (e.g. miku.gg), but it's up to you in the end. Please refer to their respective pages for a preview of the UI.","--"]},{"i":"im-using-google-colab-how-much-time-do-i-have-to-use-their-gpus","l":"I'm using Google Colab, how much time do I have to use their GPUs?","p":["The amount of time you can use when using the free plan for Google Colab highly depends on the traffic Colab recieves and your usage patterns. Google says that you can use a Colab Notebook for at most 12 hours if you don't have Compute Units. Pygmalion users tend to say it's less - usually anywhere from two to six hours."]},{"i":"how-to-avoid-reaching-the-gpu-quota","l":"How to avoid reaching the GPU quota?","p":["When you're done using Pygmalion, please terminate your Colab session! You'll waste your quota otherwise, and might find yourself unable to connect to a GPU backend the next time you login."]},{"i":"ive-out-of-gpu-time-what-do-i-do-now","l":"I've out of GPU time, what do I do now?","p":["If you've run out of GPU time, you can do nothing but wait for your quota to reset. Alternatively, you can purchase Colab Pro to increase your daily/weekly quota."]},{"i":"whats-a-compute-unit-cu-should-i-buy-one","l":"What's a Compute Unit (CU)? Should I buy one?","p":["A CU is a unit that you can buy to be sure to be granted access to a powerful GPU. Pricing for Compute Engine is based on per-second usage of the machine types, persistent disks, and other resources that you select for your virtual machines. So it's up to you."]},{"i":"ive-run-out-of-quota-but-i-cant-buy-a-compute-unit-what-do-i-do","l":"I've run out of quota, but I can't buy a Compute Unit! What do I do?","p":["In that case, you can use Kobold Horde. Generous users are donating their GPU power to the Horde so that people can use them to run language models. Pygmalion is a popular model, so you'll always find people hosting it. Keep in mind that there are more Horde users than workers, so you might have to wait a bit longer for responses."]}],[{"i":"pygmalion-7b--13b","l":"Pygmalion 7B & 13B","p":["The same procedure can be applied to LLaMA 13B for obtaining the newly released Pygmalion and Metharme 13B models. Keep in mind that the VRAM requirements for Pygmalion 13B are double the 7B and 6B variants.","Due to the LLaMA licensing issues, the weights for Pygmalion-7B and Metharme-7B are released as XOR files - which means they're useless by themselves unless you combine them with the original LLaMA weights. You will need to apply for access to the weights here. Note that you will need a .edu email address for access. For more info, please refer to the #help-and-questions channel in our Discord."]},{"l":"Models"},{"i":"chat-model-pygmalion","l":"Chat Model (Pygmalion)","p":["This model is based on Meta's LLaMA 7B and 13B, fine-tuned with the regular Pygmalion 6B dataset. This model is purely for chatting and roleplaying purposes, and the same prompt formattings you're used to will work here."]},{"i":"instruction-model-metharme","l":"Instruction Model (Metharme)","p":["This is an experimental model with a new prompt format used during training. It is capable of Chatting, RolePlaying, and Storytelling all at once. The prompting format is entirely different from the Chat Models, so Tavern will likely not work.","I'm not sure exactly which commit we used for converting the original weights - just make sure your converted hashes match the ones here"]},{"l":"Merging the weights","p":["Note that this won't work on Windows. If you're using Windows, you will need to do this with WSL2."]},{"i":"create-a-wsl2-environment-windows-only","l":"Create a WSL2 environment (Windows Only)","p":["Download Ubuntu from Microsoft Store (click on the image):","You're done! Look up \"Ubuntu\" in the Start Menu and continue to the next section."]},{"i":"convert-original-llama-weights-to-hf-optional","l":"Convert Original LLaMA weights to HF (optional)"},{"l":"Requirements","p":["python==3.10","git","This step can be skipped if you have the weights in HF format already.","Acquire the original LLaMA weights. Apply for access here. Place them in a location you can easily remember.","Open a Terminal instance, and create a clean Python 3.10 virtual environment:","I'm not sure exactly which commit we used for converting the original weights - just make sure your converted hashes match the ones here. If you find it's incorrect, please submit a PR or an issue with the correct commit hash.","Clone transformers and switch to the tested version:","Download these exact dependency versions:","Run this in root transformers repo:","Replace input_path_llama_base with the path to the \"LLaMA\" folder (this is the folder with all the model variants, including 7B, 13B, 30B, and 65B). Same thing for output_path_llama7b_hf but for the output path. Can be anywhere you want it to be."]},{"l":"Merge the weights","p":["decapoda-research's repo is severely outdated and will not work. You will need to have a set of 7B weights that have been converted in the past 2 weeks. Please apply for access to the LLaMA weights and convert them yourself.","Apply the XOR files by running the following command using the script provided in the repo:","Replace /path/to/hf-converted/llama-7b with the location of your converted LLaMA-7B model.","Download this zip file and extract its contents inside the new pygmalion-7b folder:","Pygmalion 7B JSONs","Metharme 7B JSONs"]},{"l":"7B File hashes","p":["Make sure your files match these sha256 hashes:","Merged Pygmalion-7B weights:","Merged Metharme 7B weights:","Original LLaMA 7B weights along with the tokenizer.model file (can be found in the root LLaMA directory):","LLaMA 7B weights converted to HuggingFace format:"]},{"l":"13B File Hashes","p":["Pygmalion 13B Merged Weights:","Metharme 13B Merged Weights:","Converted LLaMA 13B weights:"]}],[{"l":"Colab Notebooks","p":["Google has effectively banned all mentions of PygmalionAI in their Colabs. As a result of this, Kobold and Tavern's official Colabs will no longer work. Oobabooga's Text Generation WebUI still works, so we'll have a guide for that included here.","As of April 24th 2023, Google has banned all the mixed models notebooks, so accessing Pygmalion is even more difficult. In light of this, we have removed all Google Colab links from the docs."]}],[{"l":"Vast AI","p":["Vast AI is a cloud computing service that provides high-end consumer and datacenter GPUs for very cheap prices. You can easily rent one of their many GPUs and use them to run Pygmalion AI."]},{"i":"why-vastai","l":"Why Vast.ai?","p":["Google Colab offers free GPUs, however, Google is quite restrictive about how the end-user makes use of their notebooks. They have already banned PygmalionAI specifically from their notebooks, so Colab users will be very limited in what they can do. Due to their cheap prices, vast.ai should be quite affordable for the majority of users."]},{"l":"Get started","p":["Use this url to set up a PygmalionAI docker template. This will install all the necessary requirements to run PygmalionAI via a docker image.","After opening the URL, you'll be prompted to sign up if it's your first time using vast.ai's services.","You'll be greeted with the Instances page, where you can select which GPU to use. For PygmalionAI, the cheaptest and best option is an NVIDIA RTX A5000 24GB, which allows for the maximum context size at 2048 tokens.","Make sure you set the Disk size to at least 40GB, and the price to the increasing order, as demonstrated in the screenshot above.","Assuming you've set up a billing plan, you should be able to immediately rent your desired machine and get started.","Click on \"Rent\" next to your selected GPU to get started.","Now, head on over to your Instances page and wait for your machine to setup everything for you.","Depending on the download speed of the machine you selected, it could take up to 5 minutes. If it's especially slow at 100mbps, it could potentially take 20 minutes. Make sure you select a machine with a fast internet connection.","Once it's finished loading, click on the logs button to view your remote url:","This process can take up to 3 minutes, so it's advised to close and re-open the logs window until the url shows up at the end.","Now, paste the URL in a browser tab and you're ready to use Kobold! You can load Pygmalion 6B by clicking on Models on the top-left corner of the screen, selecting the Chat models section, selecting Pygmalion 6B, and then clicking on Load. This will download the model for you. Keep in mind that the download is 16GB, but if you've chosen a machine with a fast download speed, it shouldn't take long. The steps should be familiar for users running locally, so you can also refer to the Local installation guide for Kobold.","You're now ready to use PygmalionAI! Make sure you delete (or stop, if you don't mind paying for the hourly storage charges of 0.8 cents/hr) your instance so you won't be billed when you're not using your instance. If you delete the instance, you'll have to go through the process of setting it up again, but stopping and resuming should pick up from where you left off."]},{"l":"Connecting your instance to TavernAI","p":["You can easily use TavernAI along with Kobold. Follow the instructions in the Tavern guide but instead of inputting localhost:5000/api, use your trycloudflare.com link. Assuming your URL was https://pieces-strictly-transparency-luther.trycloudflare.com/new_ui, you would paste it as https://pieces-strictly-transparency-luther.trycloudflare.com/api inside Tavern. Note that the new_ui part is removed and replaced with /api."]},{"l":"Connecting your instance to Agnaistic","p":["You can easily use Agnaistic along with Kobold. Follow the instructions in the Agnaistic Guide but instead of inputting localhost:5000/, use your trycloudflare.com link. Assuming your URL was https://pieces-strictly-transparency-luther.trycloudflare.com/new_ui, you would paste it as https://pieces-strictly-transparency-luther.trycloudflare.com inside Agnaistic. Note that the new_ui part is removed."]}],[{"l":"Kobold Horde","p":["Kobold Horde is a massive crowdsourced distributed inference cluster for AI models. It allows users running the model locally to host it for other users via the AI Horde website.","In this section, you'll learn how to both host Pygmalion on Kobold Horde or use one of the hosted instances of Pygmalion for free.","You can use the horde anonymously, but without kudos, you'll be placed at the bottom of the queue so it might take too long for a response. You can login to the horde to receive/spend kudos by acquiring an API Key.","This key will be permanently yours. It's as important as a password, so never give it to anyone if they ask for it. For transferring kudos, only your username is required (e.g. alpin#13095)."]},{"l":"How to use Pygmalion on the Horde","p":["Pygmalion is a popular model, so you'll always find at least a few workers (people hosting the model for the public) on the Horde. You can either use the official Kobold Horde website, but it's recommended to use a UI such as SillyTavern or Agnaistic."]},{"l":"SillyTavern","p":["Follow the installation instructions here. Once you launch SillyTavern, click on the red plug icon, choose KoboldAI from the \"API\" drop-down menu, check the \" Use Horde\" option, paste your Horde API Key in the \" API Key\" input box, and then select your desired model from the models list. You're done! You can now start chatting. Import a character by opening \"Character Management\", clicking on the \"Import Character from File\" button, and selecting your character. Character cards can be acquired from the Discord or the unofficial booru."]},{"l":"Agnaistic","p":["You can install Agnaistic on your PC, but it's recommended to use their website, which works on mobile phones as well.","Navigate to the Agnaistic website here:","Agnaistic Website","Once you're in, it's recommended to login so your chats and logs are saved. To access the horde, head over to Settings, use the \"Horde\" preset, and enter your API key. Select Pygmalion from the \"Horde Model\" drop-down list. Then Update Settings to save your configuration. You can now start chatting!"]},{"l":"How to acquire kudos","p":["You can earn kudos by either hosting a model on the horde yourself - which will net you the highest amount of kudos - or rate images for the Stable Horde to earn an average of 8 kudos per rating. Each rating can take up to 3 to 5 seconds. To start rating images, head over to the Artifical Art website, enter your API Key in the \"Options\" menu, and in the \"Generate\" menu, select the \"Rating\" section.","Alternatively, you can ask other users to transfer kudos for you."]},{"l":"How to host Pygmalion on the Horde","p":["You'll need to run Pygmalion with KoboldAI. The 4bit KoboldAI will also work fine. Make sure you're in the New UI (you can access it by adding /new_ui at the end of your Kobold URL or clicking on the Try New UI button). Load the model you want to host, head over to \"Settings\" and set your Horde Worker's name (arbitrary name) and the Horde API Key (this will be the API key you get when registering).","Then head over to home, and toggle the \"Share with Horde\" switch.","Have fun hosting!"]}],[{"l":"Overview","p":["Due to recent strides in the AI field, it is now possible to run Large Language Models entirely on CPU with surprisingly decent speeds. One such library is GGML. This makes it possible to run Pygmalion 6B even on mobile phones! (Provided you have at least 8GB of RAM.) In this section, you'll find guides for CPU inferencing, applicable to both PCs (irregardless of your Operating System) and Mobile Phones (Android only for now).","The system requirement for Pygmalion C++ is about 4-6GB of RAM. This isn't VRAM, and you don't even need to have a GPU at all!","Pygmalion C++ will take a few minutes to process your input tokens when you first load a character. This is normal and won't be repeated every time you type in a prompt for your bot.","Please continue to the next section."]}],[{"l":"koboldcpp","p":["Initially, the only way to run Pygmalion on CPU was through this repo: AlpinDale/pygmalion.cpp. However, thanks to the efforts of concedo from the KoboldAI team, we now have an easy-to-run executable for windows, and a compilable UI for Linux/MacOS/Android users."]},{"l":"6B model","p":["Pygmalion-6B 4bit (concedo)","If the model above doesn't work for any reason, you can download the old one instead:","Pygmalion-6B 4bit (alpindale)"]},{"l":"7B model","p":["You will need to obtain your copy of Pygmalion 7B by merging the original LLaMA (converted to HF format) with the XORed files in the PygmalionAI/pygmalion-7b repo. You will then have to convert the model to GGML format, and then quantize it down to 4bit/5bit."]},{"l":"Linux"},{"l":"Requirements","p":["git","python"]},{"l":"Converstion","p":["Obtain the merged weights.","Create a new folder and place the Pygmalion-7B weights (with its folder) in there.","Open a PowerShell/Terminal instance inside the folder and run:","Convert to GGML format by running this:","This will produce a 32-bit GGML model. We convert to 32-bit instead of 16-bit because the original Pygmalion-7B model is in BFloat-16 format, and direct conversion to FP-16 seems to damage accuracy.","Compile the quantize program:","Quantize the model:","This will output the model in the q4_2 format, which is the current best 4bit format for GGML. You can also q5_1 which has slightly better accuracy. Here's the full list of conversion types; note that q8_0 does not work with koboldcpp:"]},{"l":"Windows Guide","p":["Running on Windows is exceedingly simple. You can download the .exe file from here, and the Pygmalion 6B model linked above.","Once you have both files downloaded, all you need to do is drag the pygmalion-6b-v3-q4_0.bin file and drop it into koboldcpp.exe file. This will load the model and start a Kobold instance in localhost:5001 on your browser. You can also simply double-click on the .exe file."]},{"l":"MacOS Guide","p":["The ggml library is primarily designed for Apple Silicon, and as such, it runs the best on native Mac. There are no pre-compiled binaries available for MacOS, but compiling it yourself should be very simple."]},{"i":"requirements-1","l":"Requirements:","p":["Python","C toolchain"]},{"i":"install-the-requirements","l":"Install the requirements:","p":["Open a terminal.","Paste this command and press enter to execute it:","Install Python from here."]},{"i":"installation","l":"Installation:","p":["Download the latest release file from here. You're looking for the Source code (zip) file.","Extract it anywhere you want.","Open a Terminal inside the extracted folder.","Type in make and let it compile the program.","Once it's finished, download the model and place it inside the koboldcpp folder.","Run the model with:","Open your browser and navigate to http://127.0.0.1:5001.","You're done!"]},{"l":"macOS Quickstart","p":["If you want to launch it again later, all you need to do is open a terminal inside the koboldcpp folder, and run:","Make sure you bookmark http://localhost:5001 so you can easily open the Koboldcpp interface."]},{"l":"Linux Guide"},{"i":"requirements-2","l":"Requirements:","p":["python","gcc","python-pip","git","wget"]},{"i":"installation-1","l":"Installation:","p":["Run these commands in order:","Open your browser and navigate to localhost:5001. You're all set."]},{"l":"Linux Quickstart","p":["If you want to launch it again later, all you need to do is open a terminal inside the koboldcpp folder, and running:","Make sure you bookmark http://localhost:5001 so you can easily open the Koboldcpp interface."]},{"l":"Android","p":["Your phone needs to have at least 8GB of RAM to run Pygmalion C++. Modern phones use over 4GB of RAM by themselves, so you won't be left with much room for Pygmalion C++ if you don't have at least 8GB."]},{"i":"requirements-3","l":"Requirements:","p":["Termux","Install Termux from F-Droid."]},{"i":"installation-2","l":"Installation:","p":["Open Termux.","Type in the following commands one by one and press enter to run them:","pkg update pkg upgrade","pkg install python clang python-pip git openssl","pip install psutil","git clone https://github.com/LostRuins/koboldcpp cd koboldcpp","make","pkg install wget","wget https://huggingface.co/concedo/pygmalion-6bv3-ggml-ggjt/resolve/main/pygmalion-6b-v3-ggml-ggjt-q4_0.bin","python koboldcpp.py pygmalion-6b-v3-ggml-ggjt-q4_0.bin","Switch to your browser (don't close Termux) and go to localhost:5001.","You're done!"]},{"l":"Android Quickstart","p":["If you want to launch it again, open Termux and make sure you're in the koboldcpp folder. You can confirm this by checking whether the bash prompt is ~ $ or ~/koboldcpp $. If it's the former, run cd koboldcpp. If it's the latter, continue.","Type this in to run koboldcpp again:","Now open http://localhost:5001 on your browser."]},{"l":"Connecting to Tavern","p":["You can connect Pygmalion C++ to Tavern the same way you would the regular KoboldAI. There's a guide included here."]},{"l":"Connecting to Agnaistic","p":["You can connect Pygmalion C++ to Agnaistic the same way you would the regular KoboldAI. There's a guide included here."]}],[{"l":"Overview","p":["The requirements for the 6B and 7B models can be easily met with either an RTX 3090 or a 4090. However, running at lower precision allows running on GPUs with VRAM as low as 6GB. With koboldcpp, you can even run these models entirely on CPU! This section of the docs, however, focus on running the models on GPU, so please proceed only if you have at least 6GB of VRAM. The Quickstart has guides for finding out your VRAM amount if you're unsure."]}],[{"l":"Setting up your GPU","p":["Consumer-grade GPUs are designed for graphics processing and 3D rendering - they're not specifically made for language processing models. However, GPU vendors such as NVIDIA and AMD have released toolkits for enabling deep learning computation for their GPUs. Only a handful of AMD GPUs are supported, but the majority of NVIDIA cards can be set up easily.","Keep in mind that the most important factor for AI is VRAM (video memory), not the processing power. e.g. an RTX 2070 (8GB) is better than an RTX 4050 (6GB) when it comes to AI inference.","Please refer to the links below to check if your GPU is compatible:","-","If your GPU is supported, please proceed with the installation."]},{"l":"NVIDIA","p":["NVIDIA cards are mostly supported, though it's not recommended to use GPUs with less than 8GB of VRAM.","The setup process will be different for both Windows and Linux (MacOS currently not supported)."]},{"l":"Windows","p":["You'll need to install the CUDA Toolkit to enable compute on your NVIDIA GPU. You can download them here. Note that you should install version 11.7 CUDA, as the newer or older versions have been found to not work with KoboldAI."]},{"l":"Linux","p":["CUDA installation is relatively easy for Linux (unless you're on Fedora). For this particular guide, we'll be covering two base distros - Arch Linux and Debian. More will be added later."]},{"l":"Arch Linux","p":["Install the NVIDIA drivers (if they aren't already installed):","If you're on the LTS Arch Linux Kernel, then please install nvidia-lts instead. You can find what Kernel you're using by running uname -r","Install CUDA and (optionally) cuDNN:","You're done! Verify installation by running nvcc -v."]},{"l":"Debian","p":["Install the NVIDIA drivers (if they aren't already installed):","Install CUDA:","You're done! Verify installation by running nvcc -v."]},{"l":"AMD","p":["ROCm is (currently) not supported by Windows. You cannot run any language model on Windows if you have an AMD card - this isn't a Pygmalion-only issue. You will need to either dual-boot Linux or switch to Linux entirely if you wish to run language models on your AMD GPU.","I highly recommend following this rentry by an AMD anon who managed to get Oobabooga's TextGen working on a 6900 XT GPU: https://rentry.org/eq3hg"]},{"i":"linux-1","l":"Linux","p":["The ROCm installation is only (relatively) easy on Arch Linux. For other distros, you will need to manually install the binaries by building ROCm from source (not recommended for beginners).","Installing Arch Linux is not easy for a beginner, therefore look into Manjaro or EndeavourOS- they both use an Arch Linux base."]},{"i":"arch-linux-1","l":"Arch Linux","p":["Install GPU drivers by following the official guide.","Install an AUR Helper. We'll use paru for this guide:","Install ROCm:"]},{"i":"not-using-arch-linux","l":"Not using Arch Linux?","p":["AMD has a comprehensive guide for installing ROCm. Please refer to their installation guide."]}],[{"l":"KoboldAI","p":["KoboldAI is a browser-based front-end for AI-assisted writing and chatting with multiple local and remote AI models.","KoboldAI also supports PygmalionAI - although most primarily use it to load Pygmalion, and then connect Kobold to Tavern. You can still use Kobold in its New UI with Chat mode."]},{"l":"Installation","p":["Please make sure you've read the Overview and Setting up your GPU pages first."]},{"l":"Windows","p":["Make sure you don't have a B: Drive.","Download KoboldAI and run the .exe file:","KoboldAI","When you reach this part, type 2 and press enter.","Search for KoboldAI in the Start Menu and launch it.","Don't launch KoboldAI as Administrator!"]},{"l":"Linux"},{"l":"Requirements","p":["git"]},{"i":"installation-1","l":"Installation","p":["Clone the repo:","Launch KoboldAI"]},{"l":"Using KoboldAI","p":["Once you've launched KoboldAI, click on AI on the top-left corner of the screen, then click on Chat Models and select PygmalionAI/pygmalion-6b. You can then click on Load.","If you want to use Pygmalion 7B, place your model inside KoboldAI's models folder, and select Load a model from its directory instead of Chat Models. There, you should see the 7B model in the list.","Loading 28/28 layers on GPU needs around 16GB VRAM. You can assign fewer layers based on how much VRAM your GPU has access to. Do not put any layers on Disk!"]}],[{"l":"KoboldAI 4bit","p":["KoboldAI is a browser-based front-end for AI-assisted writing and chatting with multiple local and remote AI models.","This guide is for users with less than 10GB of VRAM. Pygmalion 6B and 7B with 4bit quantization can run on GPUs with 6GB of VRAM and above.","Use the Table of Contents to navigate."]},{"l":"Requirements","p":["Please make sure you :","Have read the Overview and Setting up your GPU pages first.","Have Git installed, tutorial here.","Don't have a B: Drive (Windows only)"]},{"l":"Installation"},{"l":"Windows","p":["Download the Kobold exe cutable file from here:","KoboldAI","Run the .exe file and follow the instructions until you have an installed copy of KoboldAI. But you're not done yet!","Download this file and place it inside the newly installed KoboldAI folder:","4bit Patch","Once in the KoboldAI folder, you may simply double-click on the update-koboldai-occam-4bit.bat file and it'll patch your KoboldAI installation to support 4bit!","After you've done that, congrats! You're almost there. You need to download the model now."]},{"l":"Downloading the model","p":["In your Kobold folder, navigate to the models folder.","Right click on the models folder and select \"Open in Terminal\". You may download a model now using git clone commands. There are two models to choose from. If you run","it will download the main Pygmalion version, V3.","As an alternative, Pygmalion Version 8 Part 4 is also available for download. In comparison to V3, V8 was fine tuned on a larger dataset which according to user feedback improved coherency and general knowledge of the model at the cost of being a little less inclined to engage in NSFW roleplay. To download it, run","instead.","Your preferred model should be downloaded in your models folder. The following steps are identical no matter which model you've downloaded.","The current GPTQ implementation on Kobold needs your model name to be either 4bit.pt/ 4bit.safetensors or 4bit-128g.pt/ 4bit-128g.safetensors. This is the file, and not the folder itself! Look for the ~4GB file inside the folder you've downloaded! Please rename them appropriately:","At that point, all the installation part is done."]},{"l":"Loading the model","p":["On the Kobold URL http://localhost:5000/new_ui, click on the home tab, then on Load Model","On the list, pick the Load a model from it's directory option.","Pick the model we just downloaded, then click on load !","The model will now be loaded and be ready to be use, if you want to use it with SillyTavern or Agnaistic, please go here, or the Agnaistic Guide here."]},{"l":"Linux","p":["With Linux, you should be able to get both 8bit and 4bit support using Occam's fork. For 8bit, you will need the original model. For 4-bit, you can download any of the GPTQ quantized models."]},{"i":"requirements-1","l":"Requirements","p":["git","python==3.10.9","aria2","cmake","make","gcc"]},{"i":"installation-1","l":"Installation","p":["Clone Occam's fork of KoboldAI","Install the Kobold requirements","This is a 2.5GB download.","Navigate to your Kobold folder and right click on the models folder. Choose Actions -> Open Terminal Here. As the steps for downloading a model are the same for Windows, you may refer to this part of the guide: Downloading the model.","The model needs to be named either 4bit.pt/ 4bit.safetensors or 4bit-128g.pt/ 4bit-128g.safetensors to work. Make sure you properly rename it.","Start KoboldAI by running ./play.sh.","Navigate to the NewUI (you can simply add new_ui at the end of the URL) and activate the Experimental UI.","Click on Load Model on the left-side menu and choose the following option:","On the list, pick the Load a model from it's directory option.","Pick the model we just downloaded, MAKE SURE THE 4 BIT MODE IS ON, then click on load !","You can now run Pygmalion 6B on your low VRAM GPU. Rejoice. Pygmalion 7B will require the same process, so you shouldn't have any trouble."]}],[{"l":"TextGen WebUI","p":["Oobabooga's Text Generation WebUI is a gradio frontend for running large language models.","Unlike KoboldAI, it can be used as a standalone front-end - you can still connect to SillyTavern or Agnaistic though."]},{"l":"Automatic Installation","p":["Please make sure you've read the Overview and Setting up your GPU pages first."]},{"l":"Windows","p":["Oobabooga has generously created a one-click installation script for Text-Gen WebUI. You can download it here:","Text-Gen WebUI Windows Installer","Simply extract the oobabooga-windows.zip file and click on the start_windows.bat file. This will install everything you need.","For GPUs with low VRAM, you will need to use either 8bit or 4bit mode. For 4bit, refer to KoboldAI 4bit guide to obtain the model. Place the model inside oobabooga's models folder, and rename the large file inside your model folder to 4bit-128g.safetensors(this will vary between different quantization techniques).","To enable 8-bit mode, open the webui.py file with a text editor (notepad will work fine), and search for: # put your flags here!. The line will look like this:","Change that to this:","To enable 4-bit mode, change that line to:","Change the groupsize value and the --model_type according to the model you're loading.","If you want to, you can connect Oobabooga to SillyTavern or the Agnaistic Guide included here or the Agnaistic Guide included here."]},{"l":"Linux","p":["Oobabooga has created a one-click installer for Linux as well. Simply download the .zip file below, open a terminal instance in the extracted directory and run chmod +x install.sh and then ./install.sh.","Text-Gen WebUI Linux Installer","If you want to, you can connect Oobabooga to SillyTavern or Agnaistic."]},{"l":"Manual Installation","p":["You can also manually install the WebUI. This method is recommended because it's more fun. The following guide is applicable to both Windows and Linux, though the primary audience is Linux users."]},{"l":"Requirements","p":["python","git","miniconda"]},{"l":"Installation steps","p":["Install miniconda:","Miniconda3 Windows Miniconda 3 Linux","For Windows, it's simply a matter of double-clicking the downloaded .exe file for installation. For linux, you'll need to make it an executable by running chmod +x Miniconda3-latest-Linux-x86_64.sh and then running the installer script with ./Miniconda3-latest-Linux-x86_64.sh in your terminal.","Create a conda environment for TextGen:","If you are, replace the third command with this: pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.2","Run the web server:","By default, oobabooga's TextGen will load the first model (in alphabetical order) placed in your models folder. You can specify which model to load with the --model argument, followed by your model name as it's named in the models folder. Here's an example of how you'd run Pygmalion-6b:","python server.py --model PygmalionAI/pygmalion-6b --cai-chat --auto-devices --no-stream","The PygmalionAI/ part is for if you want to download the model first. If you already have pygmalion-6b downloaded in your models folder, then simply do --model pygmalion-6b.","Please don't forget to pass the --load-in-8bit argument too if you have a low VRAM PC! --auto-devices should take care of the memory assignment if you have less 10GB VRAM.","You can view the full list of commands here.","If you want to, you can connect Oobabooga to SillyTavern or Agnaistic."]}],[{"l":"Agnaistic","p":["Agnaistic is a user interface you can install on your computer (or use the production version) that allows you to interact with text generation AIs and chat/roleplay with the characters you or the community create. Agnaistic is forked from the official Pygmalion website UI, Galatea UI.","Agnaistic is just a UI; you will need to connect it to a backend: KoboldAI or TextGen WebUI.","Usage","Installation"]},{"l":"Usage"},{"l":"Navigation","p":["Characters","Chats","CHUB","If another Agnaistic user has invited you to a group chat this is where the invite will appear.","Important things like the Memory Book Guide, Feature Roadmap, and more","Information","Invites","List of all characters you have downloaded. You can favorite, download, edit, duplicate, and delete characters here.","List of all memory books you have downloaded. You can edit, download, and delete them here.","Memory","Metrics","On the left side of your screen (or upon clicking the burger menu if on mobile/low resolution device) you will find the different tabs for navigating Agnaistic.","Only visible to admins, show various stats like how many users are online, how many users are registered, and allows you to change a users password.","Presets","Profile","Self explanatory","Settings","Where all your presets are stored, default presets are listed here and cannot be deleted.","Where you can change various settings about adapters, how the UI looks, how images are generated, and various settings about STT/TTS.","Where you can preview/download characters and lorebooks (called \"Memory Books\" by Agnaistic.)","Where you set your profile picture, display name, and find your user ID."]},{"i":"adaptersservices","l":"Adapters/Services","p":["Adapters/Services are synonymous with the term \"backend\" in these pygmalion docs, Agnaistic supports","Horde","Novel AI","KoboldAI","OpenAI,","Scale","TextGen WebUI","Claude","Goose ai"]},{"l":"Presets","p":["Presets are a groups of settings used to prompt the backend. Each preset needs a service. Depending on what service you pick for the preset you will see different settings."]},{"l":"Group Chats","p":["Click burger menu in top right of chat window","Click Participants","If inviting user paste in User ID if inviting user (yes you have to ask them for their User ID)","If adding another character change invitation type to character and select character from dropdown","Click invite","For multi-character chats you will have to click on which character you want to respond with the bottom bar.","If you click the context menu you can turn on auto reply for a character. Only one character can be set to auto reply."]},{"l":"Installation","p":["Self hosting Agnaistic is simple. You have a couple of options:","NPM","Docker","Manually"]},{"l":"NPM Installation","p":["(Requires Node.js) Agnaistic is bundled as an NPM package and can be installed globally:"]},{"l":"Docker Installation","p":["Clone the project","git clone -b dev https://github.com/luminai-companion/agn-ai cd agn-ai","With MongoDB:","docker compose -p agnai -f self-host.docker-compose.yml up -d","Without MongoDB: docker run -dt --restart=always -p 3001:3001 ghcr.io/luminai-companion/agnaistic:latest","-dt Run the container detached","--restart=always Restart at start up or if the server crashes","-p 3001:3001 Expose port 3001. Access the app at http://localhost:3001"]},{"l":"Manual Installation","p":["Anonymous users have their data saved to the browser's local storage. Your data will \"persist\", but not be shareable between devices or other browsers. Clearing your browser's application data/cookies will delete this data.","Build and run the project in watch mode:","Build and run the project with Local Tunnel:","Do this every time you update AgnAI, just in case.","Download the project: git clone https://github.com/luminai-companion/agn-ai or download it","From inside the cloned/unpacked folder in your terminal/console:","Install MongoDB Optional","Install Node.js","Mac/Linux: npm run start","Mac/Linux: npm run start:public","npm run build:all","npm run deps","The database is optional. Agnaistic will run in anonymous-only mode if there is no database available.","This will install the dependencies using pnpm v6","Windows: npm run start:public:win","Windows: npm run start:win"]}],[{"l":"SillyTavern","p":["SillyTavern is a user interface you can install on your computer (and Android phones) that allows you to interact with text generation AIs and chat/roleplay with the characters you or the community create.","SillyTavern is just a UI; you will need to connect it to a backend: KoboldAI or TextGen WebUI.","Bubble Chat","Waifu Mode","VN mode","Background images"]},{"l":"Navigation"},{"l":"API Menu","p":["API","This is where you select the API you wish to connect SillyTavern to. The currently supported APIs are: KoboldAI, Kobold Horde, Oobabooga's TextGen WebUI, NovelAI, OpenAI, and Poe. Pygmalion is only accessible through KoboldAI, Kobold Horde, and Oobabooga. Instructions for acquring the API Key for those two are provided in their respective pages - SillyTavern itself offers guides for acquiring your API key for the other three."]},{"l":"Advanced Formatting","p":["This is where you'll find various options for a more intimate control over the model's prompting and output. You can click on the question mark next to the \"Advanced Formatting\" title in SillyTavern for an in-depth explanation of each option. For Pygmalion, this can be left mostly alone - just make sure the Tokenizer is set to \"Sentencepiece (LLaMA)\" for Pygmalion 7B/13B. For Metharme models, select \"Metharme\" from the Presets drop-down menu and you're good to go."]},{"l":"World Selector","p":["World Info Selector","You can load your World Info and Soft Prompt file here. For a more in-depth explanation, refer to our Soft Prompt and World Info pages. You can also click on the Question Mark next to each option in ST for a detailed overview."]},{"l":"User Settings","p":["You can customize the UI and basic SillyTavern behaviour here. Notable options are Waifu Mode, UI Theme Preset, Swipes, User Name, and UI Colors. The names are mostly self-explanatory - Waifu Mode crops the Chat Box so you can see your character's sprite (if you've added one)."]},{"l":"Background Images","p":["BG","You can select or add new background images here. Make sure to pick one that fits the chat's mood!"]},{"l":"Extensions API","p":["SillyTavern Extras","This menu is for SillyTavern Extras and various other extensions. You can add Expression images for your characters, use Stable Diffusion to generate images of the chat/story, TTS for making your characters talk, and image captioning so you can send images directly to your character!","You can either run SillyTavern-extras locally or use the provided Google Colab Notebook. If you went the local route, launch ST-Extras and click connect in ST. If you use Colab, copy your Remote URL and paste it in the API field, then click connect.","Stable Diffusion generations are done through either SillyTavern-extras or Stable Horde. If you're not using the extras, make sure you check the \"Use Stable Horde\" option. If you have kudos, make sure you add your Horde API Key in the API menu first."]},{"l":"Character Management","p":["Characters You can create, upload, and manage your characters here. ST by default comes with 3 characters - Aqua, Darkness, and Megumin. You can also assign tags for each character based on their character style (docs on those are currently a WIP). You can also create Group Chats here (similar to Rooms in C.AI)."]},{"l":"Presets","p":["Kobold Presets","Depending on which API you've selected, you can choose from the multiple presets provided by the ST team. You can also adjust the generation settings here (learn more about those here). Keep in mind that all current Pygmalion/Metharme models have a hard-coded context limit of 2048 tokens, so please don't unlock the context size to assign a higher value."]},{"l":"Installation","p":["Installing SillyTavern is simple. It supports the following operating systems:","Windows x64","Linux x64","macOS (Darwin x64)","Android (aarch64)"]},{"l":"Windows Installation","p":["First, you will need to install Node.js.","Just open the installer, and click on Next, Next, etc... Leave everything as default. Once Node.JS is installed, you can download Silly Tavern!","You'll need Github Desktop to clone the repository.","After installing GitHub Desktop, click on Clone a repository from the internet.... (Note that you do not need to create an account for this step)","On the menu, go into the URL tab, enter this url https://github.com/Cohee1207/SillyTavern and click clone.","You can change the Local path to change where SillyTavern is going to be downloaded.","To open SillyTavern, just go into the folder where you cloned the repository, and double click on the start.bat file.","By default, the repository will be clone here : C:\\Users[Your Account Name]\\Documents\\GitHub\\SillyTavern","If everything is working, the CMD should look like this and a SillyTavern tab should be open in your browser.","You still have to connect it to a backend API!"]},{"i":"linuxmacos-installation","l":"Linux/MacOS Installation"},{"l":"Requirements","p":["nodejs","git","Install nvm(NodeJS Version Manager) by running the following command in Terminal:","Run this to make nvm usable:","Install NodeJS by running the following command:"]},{"i":"installation-with-npx","l":"Installation (with npx)","p":["Run npx sillytavern@latest. You can re-open it anytime by running the command again. Omit the @latest part of the command to not auto-pull the latest update as that might overwrite all your previous characters and chatlogs."]},{"i":"installation-manual","l":"Installation (manual)","p":["Clone the repo","Run TavernAI"]},{"l":"Android Installation","p":["Install Termux.","Run the following commands inside Termux, in order:","Update repos/packages:","Install NodeJS and git:","Download Silly Tavern:","If you receive the above error, it means you've already downloaded SillyTavern before. In that case, you can simply update it. Change directories to SillyTavern by running cd SillyTavern, and then updating it by running git pull.","Run SillyTavern:","You can run SillyTavern again by simply opening Termux and entering the command above!"]},{"l":"Connect to Horde","p":["Recommended for Phone Users"]},{"l":"Connect SillyTavern","p":["If you're running KoboldAI locally, all you need to paste in there is http://localhost:5000/api. If you're using Google Colab, copy your remote URL instead. If it ends with a # or new_ui, remove them and replace them with /api. If they don't, simply adding /api will suffice.","You can now start using SillyTavern! SillyTavern uses Character Cards, which are images (PNG and WEBP) that can be imported to Tavern as Characters. You can also find new characters in the unofficial Discord server."]}],[{"l":"Soft Prompts"},{"i":"what-are-soft-prompts-sp-and-do-i-need-them","l":"What are Soft Prompts (SP) and do I need them?","p":["A soft prompt is a file that modifies the style and behavior of the language model. It can create a bias towards the tone or the style of an author or series, and the model can then use this information to generate an output that fits your purposes for a specific bot(s). Even if they can help the model to internalize lore, facts or information, they are not suited for it. SPs do not re-train model and as such, no new knowledge is acquired. If you're interested in how softprompts work on a technical level, please refer to [this section]"]},{"l":"Using Soft Prompts"},{"l":"KoboldAI"},{"l":"Downloading and placing the SP","p":["First, download your SP of choice from either the Discord or this rentry a generous anon is maintaining.","For using a Soft Prompt, the method is slighlty different depending on the way you are running the model."]},{"l":"Locally","p":["If you're running KoboldAI locally on your PC (not using Google Colab), place the .zip file (don't extract!) inside the \\KoboldAI\\softprompts folder."]},{"l":"Google Colab","p":["Make sure to check the use_google_drive option and granting access to your Google Drive in the pop-up.","Then on the left of the screen click on the folder icon. Navigate through the files to find the \\MyDrive\\KoboldAI\\softprompts folder. Drag and drop your .zip file (don't extract!) inside."]},{"l":"Loading the Soft Prompt","p":["Once it's done, go into the Kobold UI, make sure the model is loaded (this is done automatically on Colab), and click on the Soft Prompt tab.","For Google Colab user, please click on the remote URL provide by the colab to reach that UI!","Then select your Soft Prompt and click on the load button.","If your Soft Prompt is loaded, the light bulb should be green!"]},{"i":"how-do-soft-prompts-work","l":"How do Soft Prompts work?","p":["Refer to this paper for a more detailed overview, and this wiki page for a more in-depth and human-readable explanation.","Soft prompting is an alternative to model fine-tuning that freezes the weights of a model and updates the parameters of a prompt to adapt the model to different downstream tasks. This results in a \"soft prompt\".","The advantage of soft prompting is that it allows the same model to be used for mutliple tasks by appending the appropriate prompts at inference time. This makes batching across different tasks easier. Soft prompts trained for a single model across multiple tasks will often be of the same token length. The vectors of values of the tokens can be considered as model parameters, and the model can be further trained by adjusting only the weights of these prompts.","Prompt tuning performs better with larger models, and using more than 20 tokens doesn't yield significant performance gains.","TL;DR, Soft prompts are learnable vectors that are concatenated to the input text and optimized end-to-end over a training dataset. Mixtures of soft prompts can be used to query LMs, allowing us to learn which prompts are most effective and how to ensemble them."]},{"i":"how-do-i-create-a-soft-prompt","l":"How do I create a Soft Prompt?","p":["Creating a softprompt for a a GPT-J model needs several times the computation power it takes to inference the model. KoboldAI has a Colab Notebook. As of now (2023-04-22), Colab's TPUs have a driver issue and so the Easy Tuner won't work. You can instead refer to this repo for GPU tuning - though you'll likely need a powerful GPU."]}],[{"l":"LoRA","p":["Low-Rank Adaptation (LoRA) is a paradigm of natural language processing (NLP) that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. Compared to GPT-175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirements by 3 times. Refer to the paper below:","LoRA: Low-Rank Adaptation of Large Language Models"]},{"l":"Low-Rank Adaptation","p":["LoRAs are preferred when the traditional fine-tuning methods prove to be too computationally expensive. As demonstrated by Edward Hu et al., training a LoRA is at least 3 times less resource intensive as a native fine-tune. This can be further reduced with INT4 LoRA tuning - made possible by GPTQ.","Some of the key advantages LoRAs possess are:","A pre-trained model can be shared and used to build many small LoRA modules for different downstream tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices A and B as demonstrated below, reducing the storage requirement and task-switching overhead significantly.","Figure 1","LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times (over 10 times if used in conjunction with GPTQ) when using adaptive optimizers since we don't need to calculate the gradients or maintain the optimizer states for most parameters. Instead we only optimize the injected, much smaller low-rank matrices.","The simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, instroducing no inference latency compared to a fully fine-tuned model, by construction.","LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning and prompt-tuning.","LoRAs were originally designed for LLMs, but they can be used for other objectives, such as diffusion models. The docs here will focus on the language modeling objective of LoRAs. We try to maximize the likelihood of a predicted word given the previous words in a sentence. Imagine we're given a GPT model \\Rho_\\Phi(y|x) which is parametrized by \\Phi. For example, \\Rho_\\Phi(y|x) can be a generic model based on the Transformers architecure. Now imagine we will need to adapt this large model to different downstream tasks, such as summarization, machine reading comprehension, and natural language to SQL (NL2SQL). We'll represent each downstream task by a training dataset of context-target pairs: \\large {Z=\\text{\\textbraceleft}(x_i,y_i)\\text{\\textbraceright}}_{i = 1,...N} where both x_i and y_i are sequences of tokens. For example, in summarization, x_i is the content of the article and y_i is its summary. During full fine-tuning, the model is initialized to pre-trained weights \\Phi_0 and updated to \\Phi_0 + \\Delta\\Phi by repeatedly following the gradient to maximize the conditional language modeling objective:","One of the main disadvantages for full fine-tuning is that for each downstream task, we learn a different set of parameters \\Delta\\Phi, whose dimensions |\\Delta\\Phi| equals |\\Phi_0|. If the model is large (such as GPT-3 with 175 Billion parameters (|\\Phi_0|)), storing and running independent instances of the model for different tasks would be extremely difficult.","LoRAs aim to solve this exact problem. The task-specific parameter increment \\Delta\\Phi = \\Delta\\Phi(\\Theta) is encoded by a much smaller-sized set of parameters \\Theta with |\\Theta| \\ll |\\Phi_0|. The task of finding \\Delta\\Phi then becomes optimizing over \\Theta:","LoRA uses a low-rank representation to encode \\Delta\\Phi that's both compute and memory efficient. When the model is GPT-3 175B, the number of trainable parameters |\\Theta| can be as small as 0.01% of |\\Theta_0|!"]},{"i":"how-does-it-work","l":"How does it work?","p":["A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers ususally have full-rank. When adapting a specific task, GPT models have a low \"instrisic dimension\" and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, Edward Hu et al. hypothesized that the updates to the weights also have a low \"instrinsic rank\" during adaptation. For a pre-trained weight matrix W_0 \\in \\R^{d x k}, we constrain its update by representing the latter with a low-rank decomposition W_0 + \\Delta W = W_0 + B A are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For h = W_0x, our modified forward pass yields:","Edward Hu et al. illustrate the reparametrization in Figure 1. They used a random Gaussian initialization for A and zero for B, so \\Delta W = B A is zero at the beginning of the training. We then scale \\Delta W_x by \\large{\\frac{\\alpha}{r}}, where \\alpha is a constant in r. When optimizing with Adam, tuning a is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set a to the first r we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary r. This method introduces no additional inference latency. When deployed in production, we can explicitly compute and store W = W_0 + B A and perform inference as usual. Note that both W_0 and B A are in \\R^{d x k}. When we need to switch to another downstream ask (switch LoRA adapters), we can recover W_0 by subtracting B A and then adding a different B' A', a quick operation with very little memory overhead."]},{"i":"what-are-the-benefits-of-lora","l":"What are the benefits of LoRA?","p":["The most significant benefit comes from the reduction in memory and storage usage. For a large model, such as GPT-J, trained with Adam, we reduce that VRAM usage by up to 2/3 if r \\ll d_{model} as we do not need to store the optimizer states for the frozen params. On GPT-3 175B, we can reduce the VRAM consumption during training from 1.2TB to 350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000x (from 350GB to 35MB). This allows us to train with significantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for creation of many customized models that can be swapped in and out on the fly.","LoRA also has its limitations. For example, it's not straightforward to batch inputs to different tasks with different A and B in a single forward pass, if one chooses to absorb A and B into W to eliminate additional latency. Though it's possible to not merge the weights and dynamically choose the LoRA modules to use for samples in a batch for scenarios where latency is not very important."]},{"i":"how-do-i-train-a-lora","l":"How do I train a LoRA?","p":["The xTuring repository contains code for both LoRA and full fine-tuning of LLMs such as LLaMA, GPT-J, and more. The current GPT-J (Pygmalion 6B) training code is only compatible with Alpaca and GPT4All dataset formats, so it will likely be useless for most users. There's currently work being done on creating INT4 LoRA training code for GPT-J, so please be patient and keep an eye out for any updates here."]},{"i":"how-do-i-load-a-lora","l":"How do I load a LoRA?","p":["First, choose what LoRA you want to use. We have a list in the Discord Server. The steps will differ for both Kobold and Oobabooga (TextGen WebUI)."]},{"i":"oobabooga-textgen-webui","l":"Oobabooga (TextGen WebUI)","p":["Currently, the easiest way to load a LoRA is via Oobabooga. Open a PowerShell/Terminal instance (press Shift + Righ-Click inside the folder and not on a file, then select \"Open in PowerShell), and run this command:","Replace tloen/alpaca-lora-7b with the name of the LoRA repo you want. For example, if the link in the Discord post for the LoRA says https://huggingface.co/nomic-ai/gpt4all-lora, then you'll have to put nomic-ai/gpt4all-lora in there instead.","Then you can launch Oobabooga normally and then select the LoRA from the \"Parameters\" tab."]},{"l":"KoboldAI","p":["To use a LoRA with KoboldAI, you will need to merge the LoRA with the base model first. First, grab the LoRA you want from the Discord Server. You can use git lfs to download the LoRA. Place the downloaded LoRA (if you're on Windows, the default download path is C:\\Users\\username\\) inside KoboldAI's models folder.","Then grab the merge script from here and place it in the models folder of your KoboldAI directory.","Assuming your Pygmalion model folder is named \"pygmalion-7b\", open a PowerShell/Terminal instance inside the models folder (on Windows, press Shift + Right-Click inside the folder and choose \"Open in Powershell\"), and type this command in:","Replace alpaca-lora with the name of the LoRA folder!","Now, you can load the new model from the pygmalion-7b-alpaca-lora folder, or whatever else you decided to name it."]},{"l":"koboldcpp","p":["Using LoRAs with koboldcpp requires converting the LoRA to the ggml format first. To do this, grab the LoRA you want from the Discord Server. You can use git lfs to download the LoRA.","Download the lora-to-ggml script from the llama.cpp repo, and place as the LoRA folder, but not inside it.","Then you can convert the LoRA using this command:","Replace alpaca-lora with the name of the LoRA folder you've downloaded. Same for the output folder.","Place the converted folder in a path you can easily remember, preferably inside the koboldcpp folder (or where the .exe file is for windows).","Then you can run this command:","Replace the alpaca-lora-ggml with the LoRA folder name. You can add any other arguments you want aside from these."]},{"l":"KoboldAI 4bit","p":["There's no way to directly use a LoRA in Kobold, not even in the 4bit branch. You will need to merge it with the base model, and then convert it to 4bit. The conversion process for 7B takes about 9GB of VRAM so it might be impossible for most users. If you still want to attempt it, follow the steps for KoboldAI until you get a merged model. Then use the GPTQ-for-LLaMA repo to convert the model to 4bit GPTQ format. There are guides in the repo on how to do that."]}],[{"i":"whats-git-and-do-i-need-it","l":"What's Git and do I need it?","p":["Git is a popular version control system used by software developers to manage their code. It allows developers to track changes to their code over time, collaborate with others, and revert back to previous versions if needed. Git gives you access to useful commands, such as git clone. You can use it to create a local copy of a remote repository, including all its files and history, on your local machine. The most useful command for an end-user is git pull, which allows you to update a cloned repository easily without having to re-download the zip file and merging it into your cloned folder.","Many of our tutorials use Git, so you should consider installing it by following the instructions below.","Windows x64","MacOS (Darwin x64)","Linux x64"]},{"l":"Windows","p":["Installation is very simple. Open the Start Menu and search for PowerShell. Right-click on it and Open as Administrator. Once the blue PowerShell window pops up, simply type these two commands in order and press enter to execute them:","Install git:","Install git lfs","Close PowerShell."]},{"l":"macOS","p":["To make things easier, we will use \"Homebrew\", a commandline software installer for macOS.","First you will need to open the Terminal application on your Mac. You can find it in the \"Utilities\" folder within the \"Applications\" folder.","Then run this command in the terminal (Copy paste it in the terminal and press enter):","Once it's done, simply run brew install git","At this point, you've installed Git on your Apple Silicon. You can now verify by running this command in your terminal: $ Git --version"]},{"l":"Linux","p":["I shouldn't have to explain this to you, Linux user. git is on every package manager. However, I like extensive documentation, so I will list instructions for all distros.","If you're unsure what your distro is, run uname -v | awk '{print $4}'.","Install git and git-lfs:"]}],[{"l":"Settings and Parameters","p":["You'll find a list of all settings in KoboldAI, Oobabooga's Text Generation WebUI, SillyTavern and Agnaistic. Please use the Table of Contents to navigate through this page.","Currently you'll only find a list of settings in KoboldAI. The other UIs will be added soon but they're more or less the same."]},{"l":"KoboldAI"},{"l":"Output Length","p":["Output Length is a setting used to control the length of the output generated by the model. The output length setting will determine how long the model's reply will be based on the value you set.","For example, if you set the output length to 200, the model will generate a piece of text that is approximately 100 tokens long, depending on the specifics of the model and the input data.","The Output Length setting can be useful for controlling the amount of output generated by the model and for ensuring that the generated text is of a consistent length. A higher value will take longer to generate."]},{"l":"Temperature","p":["Temperature determines the randomness of sampling. A higher value can increase creativity, but can make the output less meaningful and coherent. Lower values will make the output more predictable, but it may become more repetitive."]},{"l":"Top P Sampling","p":["Top_P is a way of generating text with an AI model that helps to make the output more diverse and interesting. When using Top_P, you select the most likely words to come next based on their probability of occurring, but only up to a certain cumulative probability threshold. This means that instead of always selecting the most likely word, the language model can choose from a wider range of words that are still reasonably likely to occur.","For example, if you set Top_P to 0.5, the language model will only consider words that make up the top 50% of the probability distribution for the next word, rather than just the most likely word.","You can can use Top_P to make the model output more varied and creative. By not always selecting the most likely word, the language model can surprise you with unexpected but still plausible words, creating more interesting and engaging text."]},{"l":"Top K Sampling","p":["When using \"top k\", you select the top k most likely words to come next based on their probability of occurring, where k is a fixed number that you specify.","For example, if you set Top_K to 5, the language model will only consider the five most likely words to come next, rather than all the words in its vocabulary.","You can use Top_K to control the amount of diversity in the model output. By limiting the selection to a small number of the most likely words, the language model can produce more predictable and conservative text. However, by increasing k, you can allow for more unexpected and creative choices in the generated text.","Overall, Top_K can be useful for balancing the tradeoff between predictability and creativity in language model output."]},{"l":"Combining Top K with Top P","p":["Combining Top_K and Top_P is a technique called nucleus sampling or Top_P sampling with Top_K cutoff. This technique allows you to use both methods together to generate even more diverse and interesting text.","Here's how it works: first, the model uses Top_p to select the most likely words up to a certain cumulative probability threshold, as explained in the Top_P page","Then, it applies a Top_K cutoff to this set of words, selecting only the top k most likely words from the Top_P selection.","By using both techniques together, you can generate text that is both diverse and controlled. The Top_P selection ensures that the language model has the freedom to choose from a range of plausible words, while the Top_K cutoff limits the number of choices to a manageable set of highly likely words.","The specific values of Top_P and Top_K can be adjusted to achieve different levels of diversity and control in the generated text. For example, a higher Top_P value will allow for more diverse choices, while a lower Top_K value will provide more control over the output. It's important to experiment with different settings to find the right balance for your specific use case."]},{"l":"Tail Free Sampling","p":["Tail Free Sampling is a setting used to improve the consistency of the generated output. When generating new text, the model can use Tail Free Sampling to select the next word, but with less emphasis on the rare or extreme values.","The slider is a tool that allows you to adjust the amount of Tail Free Sampling used by the model. When you pick a higher value, the model will be more likely to avoid selecting the rare or extreme values, which can help to increase the consistency of the generated output. This is because the model will be working from the bottom of the probability distribution, selecting more common and likely values and trimming the lowest probability tokens."]},{"l":"Typical Sampling","p":["Typical Sampling is a setting used to control how the model selects the next token or word in the generated output. With typical sampling, the model selects the next token based on its probability distribution, meaning that more probable tokens are more likely to be selected.","For example, if the model is generating a sentence and the next probable word is \"the\", the model is more likely to select \"the\" as the next word with typical sampling. This can help to ensure that the generated output is fluent and coherent, as the model is more likely to select words that make sense in the given context."]},{"l":"Top A Sampling","p":["Top A Sampling is a way to pick the next word in a sentence based on how important it is in the context. We use a \"leader\" number between 0 and 1 to help us decide which words are most important. Top-A considers the probability of the most likely token, and sets a limit based on its percentage. After this, remaining tokens are compared to this limit. If their probability is too low, they are removed from the pool. Increasing A results in a stricter limit. Lowering A results in a looser limit.","This means that if the top token has a moderate likelihood of appearing, the pool of possibilities will be large. On the other hand, if the top token has a very high likelihood of appearing, then the pool will be 1-3 tokens at most. This ensures that structure remains solid, and focuses creative output in areas where it is actually wanted."]},{"l":"Repetition Penalty","p":["Repetition Penalty is a setting used to control how often the model repeats certain words or phrases in the generated output. When the repetition penalty is set to a higher value, the model is less likely to repeat the same word or phrase multiple times in the output."]},{"l":"Repetition Penalty Range","p":["If set higher than 0, repetition penality only applies to the last few tokens of the story rather than to the entire story. The slider controls the amount of tokens at the end of your story to apply it to."]},{"l":"Repetition Penalty Slope","p":["If both this setting and Repetition Penality Range are set higher than 0, the repetition penality will be apply more strongly on tokens that are close to the end of the story. High values will result in the repetition penalty difference between the start and the end of your story being more apparent."]},{"l":"Alt Rep Pen","p":["Alt Rep Pen applies Repetition Penalty as a logarithmic modifier rather than a linear modifier. This means that the penalty will be smaller for the first few repetitions and will increase more slowly as the number of repetitions increases.","In simpler terms, the more your model repeats a token, the more severe the penalty becomes."]},{"l":"Context Tokens","p":["Number of tokens to submit to the model for sampling/that are used to generate the output. Make sure this is higher than \"Output Length\". Higher values increase the memory but it also increases the usage of VRAM/RAM."]},{"l":"Gens Per Action","p":["The number of outputs the model will generate at once. Use this setting if your responses are too short; it will make the bot generate twice in order. Increases VRAM/RAM usage."]},{"i":"wi-depth-world-info","l":"WI Depth (World Info)","p":["Number of actions (Input/Output) from the end of the story to scan for World Info keys.","Excerpt from the KoboldAI wiki:","World Info is where you can flesh out the details of your wider world. The engine will help conserve tokens in your context by only inserting World Info entries when their keywords are mentioned in the actual story text. However, they are inserted toward the top, after the Memory but before the actual story text, so they have a moderate effect on what the AI generates.","World Info entries are like an encyclopedia entry, providing a succinct overview of the most relevant information about whatever topic - characters, species, places, items, etc...","The AI doesn't see the key word or title of the World Info entry \"behind the scenes\", so the actual text should be an entirely self-contained description. For example, if you have a World Info entry titled \"Director Abrams\", the entry should say \"Director Abrams is the executive and governor of the colony\" rather than just \"the executive and governor of the colony\", because the AI will only see the entry.","Use World Info entries that cross reference each other appropriately to create a more dynamic and interactive world. If your \"combat android\" World Info entry mentions that they carry plasma rifles, then create a short \"plasma rifle\" entry separately. If you create a \"Admin Tower\" entry that mentions containing a command center, computer core, and offices, create distinct \"command center\", \"computer core\", and \"offices\" entries that mention being located in the Admin Tower.","It helps to use many key words that might pull in a World Info entry whenever it might be appropriate, rather than just its proper name(s).","If you have a rich tapestry of interconnected World Info entries with many relevant keywords, you may find that multiple World Info entries are getting pulled into the context at any given time. So it's good to keep them fairly short (50 tokens for minor stuff, 100 tokens for significant characters, no more than 150 for major keystones of the setting)."]},{"l":"Prefilled Memory","p":["If you create a \"Random Story\" by clicking on the \"New Game\" button with this setting enabled, the memory of the current story will be loaded into the memory of the new Random Story."]},{"l":"Chat Mode","p":["This mode optimizes KoboldAI for chatting/using conversational models. Use this mode for Pygmalion or other chatbot models."]},{"l":"Adventure Mode","p":["Turn this on if you are playing a \"Choose Your Adventure\" model. Does not apply to Pygmalion."]},{"i":"dynamic-wi-scan-world-info","l":"Dynamic WI Scan (World Info)","p":["Scans the AI's output for World Info keys as it is generating the output. If you disable this, KoboldAI will only scan the output after the bot has finished generating the text."]},{"l":"Token Probabilities","p":["Add a context menu beside the output showing what other words were considered as it was generated."]},{"l":"Token Streaming","p":["Shows tokens as they are generated. Creates a similar effect to when other models like ChatGPT or Character.AI display their output in real-time."]}],[{"l":"Overview","p":["This section is a work in progress, and will be updated regularly.","This section of the docs, though unrelated to Pygmalion, aims to provide an introduction to key concepts, algorithms, and theoretical results in machine learning. The various sections concentrates on transformer and other probabilistic models for supervised and unsupervised learning problems. You will be introduced to fundamental concepts and algorithms by building on first principles, while also being exposed to the literature, within a unified notation and mathematical framework. The sections are organized according to clearly defined categories, such as discriminative and generative models, frequentist and Bayesian approaches, exact and approximate inference, as well as directed and undirected models. This part of the docs is meant as an entry point for researchers and learners with a background in probability and linear algebra."]},{"l":"Sources","p":["Mostly this article from the excellent Machine Learning Master website."]}],[{"l":"Notation","p":["The notations commonly used in Machine Learning are of paramount importance. Knowing how to read mathemetical notations can be the difference between understanding a research paper or leaving it utterly confused. This section will attempt at explaining the commonly used notation systems in ML papers."]},{"l":"Basic Arithmetic Notation","p":["Addition: 1 + 1 = 2","Subtraction: 2 - 1 = 1","Multiplication: 2 \\cdot 2 = 4","Division: 2/2=1","Basic mathematical operations, as listed above, have a sister operation that performs the inverse operation. For example, subtraction is the inverse of addition, and division is the inverse of multiplication."]},{"l":"Algebra","p":["Algebra largely deals with variables - data of unknown value - usually represented by Roman or Greek letters."]},{"l":"Multiplication Notation","p":["All of the following can represent multiplication:","c = a x b","c = a \\cdot b","c = a * b","c = ab","A lack of notation represents multiplication."]},{"l":"Exponents and roots","p":["An exponent represents a number multiplied by itself n times, where n is the exponent.","2^3 = 2 * 2 * 2 = 8","The above is written as 2^3 in plaintext.","Square root is the inverse of an exponent.","\\sqrt{4} = 2","This is essentially the expression 2^2 = 4. A root \\sqrt{} with no index is a square root, which can also be written as \\sqrt[2]{}. In the expression \\sqrt{4}, we must find a value that, when raised to the power of 2(since we're using a square root), would give us 4. This answer is naturally 2, since 2^2 = 4.","\\sqrt[3]{8} = 2","This represents the expression 2^3 = 8 in root form."]},{"l":"Logarithms and e","p":["When we raise 10 to an integer exponent, we call this an order of magnitude:","10^2 = 10 * 10 or 100","Reversing that operation is done by calculating the logarithm of the result 100 assuming a base of 10.","\\log{10}(100) = 2","2^6 = 64","\\log{2}(64) = 6","Another popular logarithm is with the natural Euler's number ( e) base, a value with infinite precision.","e = 2.71828...","Raising Euler's number of a power is called a natural exponential function:","e^2 = 7.38905...","Inverse of it:","\\ln(7.38905...) = 2"]},{"l":"Greek Letters","p":["Refer to this wikipedia page for a full list."]},{"l":"Sequence Notation","p":["A sequence can be an array of data or a list of items."]},{"l":"Indexing","p":["A key to reading notation for sequences is the notation of indexing elements in the sequence. The notation will specify the beginning and end of the sequence, as as 1 to n, where n will be the extent or length of the sequence.","Items in the sequence are indexed by a variable such as i, j, and k as a subscript. For example, a_i is the i^{th} element of the sequence a.","If the sequence is two-dimentional (2D), two indices will be used. For example: b_{i,j} is the i,j^{th} element of the sequence b."]},{"l":"Sequence Operations","p":["As with most mathematical concepts, operations can be performed on a sequence. Two operations, addition and multiplication, are so common that they have their own shorthand: the sum and the multiplication."]},{"l":"Sequence Summation","p":["The sum over a sequence is denoted as the uppercase Greek letter Sigma \\Sigma. It's specified with the variable and start of the sequence summation below the sigma symbol (i.e. i = 1) and the index of the end of the summation above the sigma (e.g. n).","\\huge{\\displaystyle\\sum_{i=1}^n a_i}","This is the sum of the sequence a starting at element 1 to element n."]},{"l":"Sequence Multiplication","p":["This operation is denoted as the uppercase Greek letter pi \\Pi. It's specified the same way as the sequence summation with the beginning and end of the operation below and above the letter respectively.","\\huge{\\displaystyle\\prod_{i=1}^n a_i}"]},{"l":"Set Notation","p":["A set is a group of unique items."]},{"l":"Set of Numbers","p":["A common set is a set of numbers, such as a term defined as being within the set of integers or real numbers.","Some common sets are:","Set of all natural numbers: \\N","Set of all integers: \\Z","Set of all real numbers: \\R"]},{"l":"Set Membership","p":["Set membership is denoted by a symbol that looks similar to an uppercase E:","\\huge{a \\in \\R}","The above notation means that a is defined as being a member of the set \\R, or the set of real numbers.","There's also set operations; two common ones are:","Union, or aggregation: A \\cup B","Intersection, or overlap: A \\cap B","Learn more about sets on Wikipedia."]},{"l":"Other Notation","p":["There's various other notations you may come across. Here's some of the more prominent ones.","It's common to define a method in the abstract and then define it again as a specific implementation with a separate notation. For example, if we're estimating a variable x, we may represent it using a notation that modifies the x; for example:","x-bar: \\large{\\bar{x}}","x-prime: \\large{x^{\\prime}}","x-hat: \\large{\\hat{x}}","x-tilde: \\large{\\tilde{x}}","The same notation may have a different meaning in a different context, such as use on different objects or sub-fields of mathematics. For example, a common point of confusion is |x|, which, depending on the context, can mean:","|x|: The absolute or positive value of x.","|x|: The length of vector x.","|x|: The cardinality of the set x."]},{"l":"Extended Tips"},{"l":"Think about the author","p":["Humans wrote the book or the paper you're reading, so they're prone to errors, omissions, or making things confusing because they might not fully understand what they are writing. Relax the constraints of the notation you're reading and think about the intent of the author. What are they trying to get across to you?"]},{"l":"Check Wikipedia","p":["Wikipedia has a list of notations which could help you out. You can try out these two articles:","List of mathematical symbols","Greek letters used in mathematics, science, and engineering"]},{"l":"Sketch in Code","p":["Mathematical operations are functions on data. Map everything you're reading to psuedo-code with variables, for-loops, and more. You could even use a scripting language, along with a small array of contrived data or even an Excel (or LibreOffice Calc!) spreadsheet."]},{"l":"Seek Alternatives","p":["If a paper is too confusing, read explanations or gentler introductions to the topic from another paper, or an online article."]},{"l":"Ask questions","p":["Quora is a great place to ask your questions. The math nerds there will be more than happy to explain things to you, so long as you ask your questions properly. You can also try AI services such as ChatGPT to explain things to you."]},{"l":"Summary","p":["In this page, you've discovered the basics of mathematical notation that you may come across when reading the descriptions of techniques in machine learning.","Specifically, you've learned:","Notation for arithmetic, including variations of multiplication, exponents, roots, and logarithms.","Notation for sequences and sets, including indexing, summation, and set membership.","Tips on how to get help if you're struggling with notations."]}]]