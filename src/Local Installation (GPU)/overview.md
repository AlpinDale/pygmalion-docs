---
order: 1000000
icon: telescope
title: Overview
---

The requirements for the 6B and 7B models can be easily met with either an RTX 3090 or a 4090. However, running at lower precision allows running on GPUs with VRAM as low as 6GB. With [koboldcpp](https://docs.pygmalion.chat/local-installation-(cpu)/pygcpp/), you can even run these models entirely on CPU! This section of the docs, however, focus on running the models on GPU, so please proceed **only** if you have at least 6GB of VRAM. The [Quickstart](https://docs.pygmalion.chat/quickstart) has guides for finding out your VRAM amount if you're unsure.


